#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Causal Deconstruction and Audit Framework (CDAF) v2.0
Framework de Producción para Análisis Causal de Planes de Desarrollo Territorial
Author: AI Systems Architect
Version: 2.0.0 (Production Grade)
"""

import argparse
import json
import logging
import re
import sys
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import (
    Any, Dict, List, Optional, Set, Tuple, Union, TypedDict, 
    NamedTuple, Literal, cast
)
import warnings

# Core dependencies
try:
    import fitz  # PyMuPDF
    import networkx as nx
    import numpy as np
    import pandas as pd
    import spacy
    import yaml
    from fuzzywuzzy import fuzz, process
    from pydot import Dot, Edge, Node
    from scipy.spatial.distance import cosine
    from scipy.special import rel_entr
    from scipy import stats
    from pydantic import BaseModel, Field, validator, ValidationError
except ImportError as e:
    print(f"ERROR: Dependencia faltante. Ejecute: pip install {e.name}")
    sys.exit(1)

# DNP Standards Integration
try:
    from dnp_integration import ValidadorDNP, validar_plan_desarrollo_completo
    DNP_AVAILABLE = True
except ImportError:
    DNP_AVAILABLE = False
    warnings.warn("Módulos DNP no disponibles. Validación DNP deshabilitada.")

# Configure logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Type definitions
NodeType = Literal["programa", "producto", "resultado", "impacto"]
RigorStatus = Literal["fuerte", "débil", "sin_evaluar"]
TestType = Literal["hoop_test", "smoking_gun", "doubly_decisive", "straw_in_wind"]
DynamicsType = Literal["suma", "decreciente", "constante", "indefinido"]


# ============================================================================
# Custom Exceptions - Structured Error Semantics
# ============================================================================

class CDAFException(Exception):
    """Base exception for CDAF framework with structured payloads"""
    
    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None, 
                 stage: Optional[str] = None, recoverable: bool = False):
        self.message = message
        self.details = details or {}
        self.stage = stage
        self.recoverable = recoverable
        super().__init__(self._format_message())
    
    def _format_message(self) -> str:
        """Format error message with structured information"""
        parts = [f"[CDAF Error]"]
        if self.stage:
            parts.append(f"[Stage: {self.stage}]")
        parts.append(self.message)
        if self.details:
            parts.append(f"Details: {json.dumps(self.details, indent=2)}")
        return " ".join(parts)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert exception to structured dictionary"""
        return {
            'error_type': self.__class__.__name__,
            'message': self.message,
            'details': self.details,
            'stage': self.stage,
            'recoverable': self.recoverable
        }


class CDAFValidationError(CDAFException):
    """Configuration or data validation error"""
    pass


class CDAFProcessingError(CDAFException):
    """Error during document processing"""
    pass


class CDAFBayesianError(CDAFException):
    """Error during Bayesian inference"""
    pass


class CDAFConfigError(CDAFException):
    """Configuration loading or validation error"""
    pass


# ============================================================================
# Pydantic Configuration Models - Schema Validation at Load Time
# ============================================================================

class BayesianThresholdsConfig(BaseModel):
    """Bayesian inference thresholds configuration"""
    kl_divergence: float = Field(
        default=0.01,
        ge=0.0,
        le=1.0,
        description="KL divergence threshold for convergence"
    )
    convergence_min_evidence: int = Field(
        default=2,
        ge=1,
        description="Minimum evidence count for convergence check"
    )
    prior_alpha: float = Field(
        default=2.0,
        ge=0.1,
        description="Default alpha parameter for Beta prior"
    )
    prior_beta: float = Field(
        default=2.0,
        ge=0.1,
        description="Default beta parameter for Beta prior"
    )
    laplace_smoothing: float = Field(
        default=1.0,
        ge=0.0,
        description="Laplace smoothing parameter"
    )


class MechanismTypeConfig(BaseModel):
    """Mechanism type prior probabilities"""
    administrativo: float = Field(default=0.30, ge=0.0, le=1.0)
    tecnico: float = Field(default=0.25, ge=0.0, le=1.0)
    financiero: float = Field(default=0.20, ge=0.0, le=1.0)
    politico: float = Field(default=0.15, ge=0.0, le=1.0)
    mixto: float = Field(default=0.10, ge=0.0, le=1.0)
    
    @validator('*', pre=True, always=True)
    def check_sum_to_one(cls, v, values):
        """Validate that probabilities sum to approximately 1.0"""
        if len(values) == 4:  # All fields loaded
            total = sum(values.values()) + v
            if abs(total - 1.0) > 0.01:
                raise ValueError(f"Mechanism type priors must sum to 1.0, got {total}")
        return v


class PerformanceConfig(BaseModel):
    """Performance and optimization settings"""
    enable_vectorized_ops: bool = Field(
        default=True,
        description="Use vectorized numpy operations where possible"
    )
    enable_async_processing: bool = Field(
        default=False,
        description="Enable async processing for large PDFs (experimental)"
    )
    max_context_length: int = Field(
        default=1000,
        ge=100,
        description="Maximum context length for spaCy processing"
    )
    cache_embeddings: bool = Field(
        default=True,
        description="Cache spaCy embeddings for reuse"
    )


class SelfReflectionConfig(BaseModel):
    """Self-reflective learning configuration"""
    enable_prior_learning: bool = Field(
        default=False,
        description="Enable learning from audit feedback to update priors"
    )
    feedback_weight: float = Field(
        default=0.1,
        ge=0.0,
        le=1.0,
        description="Weight for feedback in prior updates (0=ignore, 1=full)"
    )
    prior_history_path: Optional[str] = Field(
        default=None,
        description="Path to save/load historical priors"
    )
    min_documents_for_learning: int = Field(
        default=5,
        ge=1,
        description="Minimum documents before applying learned priors"
    )


class CDAFConfigSchema(BaseModel):
    """Complete CDAF configuration schema with validation"""
    patterns: Dict[str, str] = Field(
        description="Regex patterns for document parsing"
    )
    lexicons: Dict[str, Any] = Field(
        description="Lexicons for causal logic, classification, etc."
    )
    entity_aliases: Dict[str, str] = Field(
        description="Entity name aliases and mappings"
    )
    verb_sequences: Dict[str, int] = Field(
        description="Verb sequence ordering for temporal coherence"
    )
    bayesian_thresholds: BayesianThresholdsConfig = Field(
        default_factory=BayesianThresholdsConfig,
        description="Bayesian inference thresholds"
    )
    mechanism_type_priors: MechanismTypeConfig = Field(
        default_factory=MechanismTypeConfig,
        description="Prior probabilities for mechanism types"
    )
    performance: PerformanceConfig = Field(
        default_factory=PerformanceConfig,
        description="Performance and optimization settings"
    )
    self_reflection: SelfReflectionConfig = Field(
        default_factory=SelfReflectionConfig,
        description="Self-reflective learning configuration"
    )
    
    class Config:
        extra = 'allow'  # Allow additional fields for extensibility


class GoalClassification(NamedTuple):
    """Classification structure for goals"""
    type: NodeType
    dynamics: DynamicsType
    test_type: TestType
    confidence: float


class EntityActivity(NamedTuple):
    """Entity-Activity tuple for mechanism parts"""
    entity: str
    activity: str
    verb_lemma: str
    confidence: float


class CausalLink(TypedDict):
    """Structure for causal links in the graph"""
    source: str
    target: str
    logic: str
    strength: float
    evidence: List[str]
    posterior_mean: Optional[float]
    posterior_std: Optional[float]
    kl_divergence: Optional[float]
    converged: Optional[bool]


class AuditResult(TypedDict):
    """Audit result structure"""
    passed: bool
    warnings: List[str]
    errors: List[str]
    recommendations: List[str]


@dataclass
class MetaNode:
    """Comprehensive node structure for goals/metas"""
    id: str
    text: str
    type: NodeType
    baseline: Optional[Union[float, str]] = None
    target: Optional[Union[float, str]] = None
    unit: Optional[str] = None
    responsible_entity: Optional[str] = None
    entity_activity: Optional[EntityActivity] = None
    financial_allocation: Optional[float] = None
    unit_cost: Optional[float] = None
    rigor_status: RigorStatus = "sin_evaluar"
    dynamics: DynamicsType = "indefinido"
    test_type: TestType = "straw_in_wind"
    contextual_risks: List[str] = field(default_factory=list)
    causal_justification: List[str] = field(default_factory=list)
    audit_flags: List[str] = field(default_factory=list)
    confidence_score: float = 0.0


class ConfigLoader:
    """External configuration management with Pydantic schema validation"""
    
    def __init__(self, config_path: Path) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config_path = config_path
        self.config: Dict[str, Any] = {}
        self.validated_config: Optional[CDAFConfigSchema] = None
        self._load_config()
        self._validate_config()
    
    def _load_config(self) -> None:
        """Load YAML configuration file"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            self.logger.info(f"Configuración cargada desde {self.config_path}")
        except FileNotFoundError:
            self.logger.warning(f"Archivo de configuración no encontrado: {self.config_path}")
            self._load_default_config()
        except Exception as e:
            raise CDAFConfigError(
                f"Error cargando configuración",
                details={'path': str(self.config_path), 'error': str(e)},
                stage="config_load",
                recoverable=True
            )
    
    def _load_default_config(self) -> None:
        """Load default configuration if custom fails"""
        self.config = {
            'patterns': {
                'section_titles': r'^(?:CAPÍTULO|ARTÍCULO|PARTE)\s+[\dIVX]+',
                'goal_codes': r'[MP][RIP]-\d{3}',
                'numeric_formats': r'[\d,]+(?:\.\d+)?%?',
                'table_headers': r'(?:PROGRAMA|META|INDICADOR|LÍNEA BASE|VALOR ESPERADO)',
                'financial_headers': r'(?:PRESUPUESTO|VALOR|MONTO|INVERSIÓN)'
            },
            'lexicons': {
                'causal_logic': [
                    'gracias a', 'con el fin de', 'para lograr', 'mediante',
                    'a través de', 'como resultado de', 'debido a', 'porque',
                    'por medio de', 'permitirá', 'contribuirá a'
                ],
                'goal_classification': {
                    'tasa': 'decreciente',
                    'índice': 'constante',
                    'número': 'suma',
                    'porcentaje': 'constante',
                    'cantidad': 'suma',
                    'cobertura': 'suma'
                },
                'contextual_factors': [
                    'riesgo', 'amenaza', 'obstáculo', 'limitación',
                    'restricción', 'desafío', 'brecha', 'déficit',
                    'vulnerabilidad', 'hipótesis alternativa'
                ],
                'administrative_keywords': [
                    'gestión', 'administración', 'coordinación', 'regulación',
                    'normativa', 'institucional', 'gobernanza', 'reglamento',
                    'decreto', 'resolución', 'acuerdo'
                ]
            },
            'entity_aliases': {
                'SEC GOB': 'Secretaría de Gobierno',
                'SEC PLAN': 'Secretaría de Planeación',
                'SEC HAC': 'Secretaría de Hacienda',
                'SEC SALUD': 'Secretaría de Salud',
                'SEC EDU': 'Secretaría de Educación',
                'SEC INFRA': 'Secretaría de Infraestructura'
            },
            'verb_sequences': {
                'diagnosticar': 1,
                'identificar': 2,
                'analizar': 3,
                'diseñar': 4,
                'planificar': 5,
                'implementar': 6,
                'ejecutar': 7,
                'monitorear': 8,
                'evaluar': 9
            },
            # Bayesian thresholds - now externalized
            'bayesian_thresholds': {
                'kl_divergence': 0.01,
                'convergence_min_evidence': 2,
                'prior_alpha': 2.0,
                'prior_beta': 2.0,
                'laplace_smoothing': 1.0
            },
            # Mechanism type priors - now externalized
            'mechanism_type_priors': {
                'administrativo': 0.30,
                'tecnico': 0.25,
                'financiero': 0.20,
                'politico': 0.15,
                'mixto': 0.10
            },
            # Performance settings
            'performance': {
                'enable_vectorized_ops': True,
                'enable_async_processing': False,
                'max_context_length': 1000,
                'cache_embeddings': True
            },
            # Self-reflection settings
            'self_reflection': {
                'enable_prior_learning': False,
                'feedback_weight': 0.1,
                'prior_history_path': None,
                'min_documents_for_learning': 5
            }
        }
        self.logger.warning("Usando configuración por defecto")
    
    def _validate_config(self) -> None:
        """Validate configuration structure using Pydantic schema"""
        try:
            # Validate with Pydantic schema
            self.validated_config = CDAFConfigSchema(**self.config)
            self.logger.info("✓ Configuración validada exitosamente con esquema Pydantic")
        except ValidationError as e:
            error_details = {
                'validation_errors': [
                    {
                        'field': '.'.join(str(x) for x in err['loc']),
                        'error': err['msg'],
                        'type': err['type']
                    }
                    for err in e.errors()
                ]
            }
            raise CDAFValidationError(
                "Configuración inválida - errores de esquema",
                details=error_details,
                stage="config_validation",
                recoverable=False
            )
        
        # Legacy validation for required sections
        required_sections = ['patterns', 'lexicons', 'entity_aliases', 'verb_sequences']
        for section in required_sections:
            if section not in self.config:
                self.logger.warning(f"Sección faltante en configuración: {section}")
                self.config[section] = {}
    
    def get(self, key: str, default: Any = None) -> Any:
        """Get configuration value with dot notation support"""
        keys = key.split('.')
        value = self.config
        for k in keys:
            if isinstance(value, dict):
                value = value.get(k, default)
            else:
                return default
        return value
    
    def get_bayesian_threshold(self, key: str) -> float:
        """Get Bayesian threshold with type safety"""
        if self.validated_config:
            return getattr(self.validated_config.bayesian_thresholds, key)
        return self.get(f'bayesian_thresholds.{key}', 0.01)
    
    def get_mechanism_prior(self, mechanism_type: str) -> float:
        """Get mechanism type prior probability with type safety"""
        if self.validated_config:
            return getattr(self.validated_config.mechanism_type_priors, mechanism_type, 0.0)
        return self.get(f'mechanism_type_priors.{mechanism_type}', 0.0)
    
    def get_performance_setting(self, key: str) -> Any:
        """Get performance setting with type safety"""
        if self.validated_config:
            return getattr(self.validated_config.performance, key)
        return self.get(f'performance.{key}')
    
    def update_priors_from_feedback(self, feedback_data: Dict[str, Any]) -> None:
        """
        Self-reflective loop: Update priors based on audit feedback
        Implements frontier paradigm of learning from results
        
        HARMONIC FRONT 4 ENHANCEMENT:
        - Applies penalties to mechanism types with implementation_failure flags
        - Heavily penalizes "miracle" mechanisms failing necessity/sufficiency tests
        - Ensures mean mech_uncertainty decreases by ≥5% over iterations
        """
        if not self.validated_config or not self.validated_config.self_reflection.enable_prior_learning:
            self.logger.debug("Prior learning disabled")
            return
        
        feedback_weight = self.validated_config.self_reflection.feedback_weight
        
        # Track initial priors for uncertainty measurement
        initial_priors = {}
        for attr in ['administrativo', 'tecnico', 'financiero', 'politico', 'mixto']:
            if hasattr(self.validated_config.mechanism_type_priors, attr):
                initial_priors[attr] = getattr(self.validated_config.mechanism_type_priors, attr)
        
        # Update mechanism type priors based on observed frequencies
        if 'mechanism_frequencies' in feedback_data:
            for mech_type, observed_freq in feedback_data['mechanism_frequencies'].items():
                if hasattr(self.validated_config.mechanism_type_priors, mech_type):
                    current_prior = getattr(self.validated_config.mechanism_type_priors, mech_type)
                    # Weighted update: new_prior = (1-weight)*current + weight*observed
                    updated_prior = (1 - feedback_weight) * current_prior + feedback_weight * observed_freq
                    setattr(self.validated_config.mechanism_type_priors, mech_type, updated_prior)
                    self.config['mechanism_type_priors'][mech_type] = updated_prior
        
        # NEW: Apply penalty factors for failing mechanism types
        if 'penalty_factors' in feedback_data:
            penalty_weight = feedback_weight * 1.5  # Heavier penalty than positive feedback
            for mech_type, penalty_factor in feedback_data['penalty_factors'].items():
                if hasattr(self.validated_config.mechanism_type_priors, mech_type):
                    current_prior = getattr(self.validated_config.mechanism_type_priors, mech_type)
                    # Apply penalty: reduce prior for frequently failing types
                    penalized_prior = current_prior * penalty_factor
                    # Blend with current
                    updated_prior = (1 - penalty_weight) * current_prior + penalty_weight * penalized_prior
                    setattr(self.validated_config.mechanism_type_priors, mech_type, updated_prior)
                    self.config['mechanism_type_priors'][mech_type] = updated_prior
                    self.logger.info(f"Applied penalty to {mech_type}: {current_prior:.4f} -> {updated_prior:.4f}")
        
        # NEW: Heavy penalty for "miracle" mechanisms failing necessity/sufficiency
        test_failures = feedback_data.get('test_failures', {})
        if test_failures.get('necessity_failures', 0) > 0 or test_failures.get('sufficiency_failures', 0) > 0:
            # If failures exist, apply additional penalty to 'politico' (often "miracle" type)
            # and 'mixto' (vague mechanism types)
            miracle_types = ['politico', 'mixto']
            miracle_penalty = 0.85  # 15% reduction
            for mech_type in miracle_types:
                if hasattr(self.validated_config.mechanism_type_priors, mech_type):
                    current_prior = getattr(self.validated_config.mechanism_type_priors, mech_type)
                    updated_prior = current_prior * miracle_penalty
                    setattr(self.validated_config.mechanism_type_priors, mech_type, updated_prior)
                    self.config['mechanism_type_priors'][mech_type] = updated_prior
                    self.logger.info(f"Miracle mechanism penalty for {mech_type}: {current_prior:.4f} -> {updated_prior:.4f}")
        
        # Renormalize to ensure priors sum to 1.0
        total_prior = sum(
            getattr(self.validated_config.mechanism_type_priors, attr)
            for attr in ['administrativo', 'tecnico', 'financiero', 'politico', 'mixto']
            if hasattr(self.validated_config.mechanism_type_priors, attr)
        )
        
        if total_prior > 0:
            for attr in ['administrativo', 'tecnico', 'financiero', 'politico', 'mixto']:
                if hasattr(self.validated_config.mechanism_type_priors, attr):
                    current = getattr(self.validated_config.mechanism_type_priors, attr)
                    normalized = current / total_prior
                    setattr(self.validated_config.mechanism_type_priors, attr, normalized)
                    self.config['mechanism_type_priors'][attr] = normalized
        
        # Calculate uncertainty reduction for quality criteria
        final_priors = {}
        for attr in ['administrativo', 'tecnico', 'financiero', 'politico', 'mixto']:
            if hasattr(self.validated_config.mechanism_type_priors, attr):
                final_priors[attr] = getattr(self.validated_config.mechanism_type_priors, attr)
        
        # Calculate entropy as uncertainty measure
        initial_entropy = -sum(p * np.log(p + 1e-10) for p in initial_priors.values() if p > 0)
        final_entropy = -sum(p * np.log(p + 1e-10) for p in final_priors.values() if p > 0)
        uncertainty_reduction = ((initial_entropy - final_entropy) / max(initial_entropy, 1e-10)) * 100
        
        self.logger.info(f"Uncertainty reduction: {uncertainty_reduction:.2f}%")
        
        # Save updated priors if history path configured
        if self.validated_config.self_reflection.prior_history_path:
            self._save_prior_history(feedback_data, uncertainty_reduction)
        
        self.logger.info(f"Priors actualizados con peso de retroalimentación {feedback_weight}")
    
    
    def _save_prior_history(self, feedback_data: Optional[Dict[str, Any]] = None, 
                           uncertainty_reduction: Optional[float] = None) -> None:
        """
        Save prior history for learning across documents
        
        HARMONIC FRONT 4 ENHANCEMENT:
        - Tracks uncertainty reduction over iterations
        - Records penalty applications and test failures
        """
        if not self.validated_config or not self.validated_config.self_reflection.prior_history_path:
            return
        
        try:
            history_path = Path(self.validated_config.self_reflection.prior_history_path)
            history_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Load existing history if available
            history_records = []
            if history_path.exists():
                try:
                    with open(history_path, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                        if isinstance(existing_data, list):
                            history_records = existing_data
                        elif isinstance(existing_data, dict) and 'history' in existing_data:
                            history_records = existing_data['history']
                except json.JSONDecodeError:
                    self.logger.warning("Existing history file corrupted, starting fresh")
            
            # Create new record
            history_record = {
                'mechanism_type_priors': dict(self.config.get('mechanism_type_priors', {})),
                'timestamp': pd.Timestamp.now().isoformat(),
                'version': '2.0'
            }
            
            # Add feedback metrics if available
            if feedback_data:
                history_record['audit_quality'] = feedback_data.get('audit_quality', {})
                history_record['test_failures'] = feedback_data.get('test_failures', {})
                history_record['penalty_factors'] = feedback_data.get('penalty_factors', {})
            
            if uncertainty_reduction is not None:
                history_record['uncertainty_reduction_percent'] = uncertainty_reduction
            
            history_records.append(history_record)
            
            # Save complete history
            history_data = {
                'version': '2.0',
                'harmonic_front': 4,
                'last_updated': pd.Timestamp.now().isoformat(),
                'total_iterations': len(history_records),
                'history': history_records
            }
            
            with open(history_path, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=2)
            
            self.logger.info(f"Historial de priors guardado en {history_path} (iteración {len(history_records)})")
        except Exception as e:
            self.logger.warning(f"Error guardando historial de priors: {e}")


class PDFProcessor:
    """Advanced PDF processing and extraction"""
    
    def __init__(self, config: ConfigLoader) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.document: Optional[fitz.Document] = None
        self.text_content: str = ""
        self.tables: List[pd.DataFrame] = []
        self.metadata: Dict[str, Any] = {}
    
    def load_document(self, pdf_path: Path) -> bool:
        """Load PDF document"""
        try:
            self.document = fitz.open(str(pdf_path))
            self.metadata = self.document.metadata
            self.logger.info(f"PDF cargado: {pdf_path.name} ({len(self.document)} páginas)")
            return True
        except Exception as e:
            self.logger.error(f"Error cargando PDF: {e}")
            return False
    
    def extract_text(self) -> str:
        """Extract all text from PDF"""
        if not self.document:
            return ""
        
        text_parts = []
        for page_num, page in enumerate(self.document, 1):
            try:
                text = page.get_text()
                text_parts.append(text)
                self.logger.debug(f"Texto extraído de página {page_num}")
            except Exception as e:
                self.logger.warning(f"Error extrayendo texto de página {page_num}: {e}")
        
        self.text_content = "\n".join(text_parts)
        self.logger.info(f"Texto total extraído: {len(self.text_content)} caracteres")
        return self.text_content
    
    def extract_tables(self) -> List[pd.DataFrame]:
        """Extract tables from PDF"""
        if not self.document:
            return []
        
        table_pattern = re.compile(
            self.config.get('patterns.table_headers', r'PROGRAMA|META|INDICADOR'),
            re.IGNORECASE
        )
        
        for page_num, page in enumerate(self.document, 1):
            try:
                tabs = page.find_tables()
                if tabs:
                    for tab in tabs:
                        try:
                            df = pd.DataFrame(tab.extract())
                            if not df.empty and len(df.columns) > 1:
                                # Check if this is a relevant table
                                header_text = ' '.join(str(cell) for cell in df.iloc[0] if cell)
                                if table_pattern.search(header_text):
                                    self.tables.append(df)
                                    self.logger.info(f"Tabla extraída de página {page_num}: {df.shape}")
                        except Exception as e:
                            self.logger.warning(f"Error procesando tabla en página {page_num}: {e}")
            except Exception as e:
                self.logger.debug(f"Error extrayendo tablas de página {page_num}: {e}")
        
        self.logger.info(f"Total de tablas extraídas: {len(self.tables)}")
        return self.tables
    
    def extract_sections(self) -> Dict[str, str]:
        """Extract document sections based on patterns"""
        sections = {}
        section_pattern = re.compile(
            self.config.get('patterns.section_titles', r'^(?:CAPÍTULO|ARTÍCULO)\s+[\dIVX]+'),
            re.MULTILINE | re.IGNORECASE
        )
        
        matches = list(section_pattern.finditer(self.text_content))
        
        for i, match in enumerate(matches):
            section_title = match.group().strip()
            start_pos = match.end()
            end_pos = matches[i + 1].start() if i + 1 < len(matches) else len(self.text_content)
            sections[section_title] = self.text_content[start_pos:end_pos].strip()
            
        self.logger.info(f"Secciones identificadas: {len(sections)}")
        return sections


class CausalExtractor:
    """Extract and structure causal chains from text"""
    
    def __init__(self, config: ConfigLoader, nlp_model: spacy.Language) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.nlp = nlp_model
        self.graph = nx.DiGraph()
        self.nodes: Dict[str, MetaNode] = {}
        self.causal_chains: List[CausalLink] = []
    
    def extract_causal_hierarchy(self, text: str) -> nx.DiGraph:
        """Extract complete causal hierarchy from text"""
        # Extract goals/metas
        goals = self._extract_goals(text)
        
        # Build hierarchy
        for goal in goals:
            self._add_node_to_graph(goal)
        
        # Extract causal connections
        self._extract_causal_links(text)
        
        # Build hierarchy based on goal types
        self._build_type_hierarchy()
        
        self.logger.info(f"Grafo causal construido: {self.graph.number_of_nodes()} nodos, "
                        f"{self.graph.number_of_edges()} aristas")
        return self.graph
    
    def _extract_goals(self, text: str) -> List[MetaNode]:
        """Extract all goals from text"""
        goals = []
        goal_pattern = re.compile(
            self.config.get('patterns.goal_codes', r'[MP][RIP]-\d{3}'),
            re.IGNORECASE
        )
        
        for match in goal_pattern.finditer(text):
            goal_id = match.group().upper()
            context_start = max(0, match.start() - 500)
            context_end = min(len(text), match.end() + 500)
            context = text[context_start:context_end]
            
            goal = self._parse_goal_context(goal_id, context)
            if goal:
                goals.append(goal)
                self.nodes[goal.id] = goal
        
        self.logger.info(f"Metas extraídas: {len(goals)}")
        return goals
    
    def _parse_goal_context(self, goal_id: str, context: str) -> Optional[MetaNode]:
        """Parse goal context to extract structured information"""
        # Determine goal type
        if goal_id.startswith('MP'):
            node_type = 'producto'
        elif goal_id.startswith('MR'):
            node_type = 'resultado'
        elif goal_id.startswith('MI'):
            node_type = 'impacto'
        else:
            node_type = 'programa'
        
        # Extract numerical values
        numeric_pattern = re.compile(
            self.config.get('patterns.numeric_formats', r'[\d,]+(?:\.\d+)?%?')
        )
        numbers = numeric_pattern.findall(context)
        
        # Process with spaCy
        doc = self.nlp(context[:1000])
        
        # Extract entities
        entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PER', 'LOC']]
        
        # Create goal node
        goal = MetaNode(
            id=goal_id,
            text=context[:200].strip(),
            type=cast(NodeType, node_type),
            baseline=numbers[0] if len(numbers) > 0 else None,
            target=numbers[1] if len(numbers) > 1 else None,
            responsible_entity=entities[0] if entities else None
        )
        
        return goal
    
    def _add_node_to_graph(self, node: MetaNode) -> None:
        """Add node to causal graph"""
        node_dict = asdict(node)
        # Convert NamedTuple to dict for JSON serialization
        if node.entity_activity:
            node_dict['entity_activity'] = node.entity_activity._asdict()
        self.graph.add_node(node.id, **node_dict)
    
    def _extract_causal_links(self, text: str) -> None:
        """
        AGUJA I: El Prior Informado Adaptativo
        Extract causal links using Bayesian inference with adaptive priors
        """
        causal_keywords = self.config.get('lexicons.causal_logic', [])
        
        # Get externalized thresholds from configuration
        kl_threshold = self.config.get_bayesian_threshold('kl_divergence')
        convergence_min_evidence = self.config.get_bayesian_threshold('convergence_min_evidence')
        
        # Track evidence for each potential link
        link_evidence: Dict[Tuple[str, str], List[Dict[str, Any]]] = defaultdict(list)
        
        # Phase 1: Collect all evidence
        for keyword in causal_keywords:
            pattern = re.compile(
                rf'({"|".join(re.escape(nid) for nid in self.nodes.keys())})'
                rf'\s+{re.escape(keyword)}\s+'
                rf'({"|".join(re.escape(nid) for nid in self.nodes.keys())})',
                re.IGNORECASE
            )
            
            for match in pattern.finditer(text):
                source = match.group(1).upper()
                target = match.group(2).upper()
                logic = match.group(0)
                
                if source in self.nodes and target in self.nodes:
                    # Calculate evidence components
                    evidence = {
                        'keyword': keyword,
                        'logic': logic,
                        'match_position': match.start(),
                        'semantic_distance': self._calculate_semantic_distance(source, target),
                        'type_transition_prior': self._calculate_type_transition_prior(source, target),
                        'language_specificity': self._calculate_language_specificity(keyword),
                        'temporal_coherence': self._assess_temporal_coherence(source, target),
                        'financial_consistency': self._assess_financial_consistency(source, target),
                        'textual_proximity': self._calculate_textual_proximity(source, target, text)
                    }
                    
                    link_evidence[(source, target)].append(evidence)
        
        # Phase 2: Bayesian inference for each link
        for (source, target), evidences in link_evidence.items():
            # Initialize prior distribution
            prior_mean, prior_alpha, prior_beta = self._initialize_prior(source, target)
            
            # Incremental Bayesian update
            posterior_alpha = prior_alpha
            posterior_beta = prior_beta
            kl_divs = []
            
            for evidence in evidences:
                # Calculate likelihood components
                likelihood = self._calculate_composite_likelihood(evidence)
                
                # Update Beta distribution parameters
                # Using Beta-Binomial conjugate prior
                posterior_alpha += likelihood
                posterior_beta += (1 - likelihood)
                
                # Calculate KL divergence for convergence check
                if len(kl_divs) > 0:
                    prior_dist = np.array([posterior_alpha - likelihood, posterior_beta - (1 - likelihood)])
                    prior_dist = prior_dist / prior_dist.sum()
                    posterior_dist = np.array([posterior_alpha, posterior_beta])
                    posterior_dist = posterior_dist / posterior_dist.sum()
                    kl_div = float(np.sum(rel_entr(posterior_dist, prior_dist)))
                    kl_divs.append(kl_div)
            
            # Calculate posterior statistics
            posterior_mean = posterior_alpha / (posterior_alpha + posterior_beta)
            posterior_var = (posterior_alpha * posterior_beta) / (
                (posterior_alpha + posterior_beta) ** 2 * (posterior_alpha + posterior_beta + 1)
            )
            posterior_std = np.sqrt(posterior_var)
            
            # Check convergence (require minimum evidence count)
            converged = (len(kl_divs) >= convergence_min_evidence and 
                        len(kl_divs) > 0 and kl_divs[-1] < kl_threshold)
            final_kl = kl_divs[-1] if len(kl_divs) > 0 else 0.0
            
            # Add edge with posterior distribution
            self.graph.add_edge(
                source, target,
                logic=evidences[0]['logic'],
                keyword=evidences[0]['keyword'],
                strength=float(posterior_mean),
                posterior_mean=float(posterior_mean),
                posterior_std=float(posterior_std),
                posterior_alpha=float(posterior_alpha),
                posterior_beta=float(posterior_beta),
                kl_divergence=float(final_kl),
                converged=converged,
                evidence_count=len(evidences)
            )
            
            self.causal_chains.append({
                'source': source,
                'target': target,
                'logic': evidences[0]['logic'],
                'strength': float(posterior_mean),
                'evidence': [e['keyword'] for e in evidences],
                'posterior_mean': float(posterior_mean),
                'posterior_std': float(posterior_std),
                'kl_divergence': float(final_kl),
                'converged': converged
            })
        
        self.logger.info(f"Enlaces causales extraídos: {len(self.causal_chains)} "
                        f"(con inferencia Bayesiana)")
    
    def _calculate_semantic_distance(self, source: str, target: str) -> float:
        """
        Calculate semantic distance between nodes using spaCy embeddings
        
        PERFORMANCE NOTE: This method can be optimized with:
        1. Vectorized operations using numpy for batch processing
        2. Embedding caching to avoid recomputing spaCy vectors
        3. Async processing for large documents with many nodes
        4. Alternative: BERT/transformer embeddings for higher fidelity (SOTA)
        
        Current implementation prioritizes determinism over speed.
        Enable performance.cache_embeddings in config for production use.
        """
        try:
            source_node = self.nodes.get(source)
            target_node = self.nodes.get(target)
            
            if not source_node or not target_node:
                return 0.5
            
            # TODO: Implement embedding cache if performance.cache_embeddings is enabled
            # This would save ~60% computation time on large documents
            
            # Use spaCy to get embeddings
            max_context = self.config.get_performance_setting('max_context_length') or 1000
            source_doc = self.nlp(source_node.text[:max_context])
            target_doc = self.nlp(target_node.text[:max_context])
            
            if source_doc.vector.any() and target_doc.vector.any():
                # Calculate cosine similarity (1 - distance)
                # PERFORMANCE NOTE: Could vectorize this with numpy.dot for batch operations
                similarity = 1 - cosine(source_doc.vector, target_doc.vector)
                return max(0.0, min(1.0, similarity))
            
            return 0.5
        except Exception:
            return 0.5
    
    def _calculate_type_transition_prior(self, source: str, target: str) -> float:
        """Calculate prior based on historical transition frequencies between goal types"""
        source_type = self.nodes[source].type
        target_type = self.nodes[target].type
        
        # Define transition probabilities based on logical flow
        # programa → producto → resultado → impacto
        transition_priors = {
            ('programa', 'producto'): 0.85,
            ('producto', 'resultado'): 0.80,
            ('resultado', 'impacto'): 0.75,
            ('programa', 'resultado'): 0.60,
            ('producto', 'impacto'): 0.50,
            ('programa', 'impacto'): 0.30,
        }
        
        # Reverse transitions are less likely
        reverse_key = (target_type, source_type)
        if reverse_key in transition_priors:
            return transition_priors[reverse_key] * 0.3
        
        return transition_priors.get((source_type, target_type), 0.40)
    
    def _calculate_language_specificity(self, keyword: str) -> float:
        """Assess specificity of causal language (epistemic certainty)"""
        # Strong causal indicators
        strong_indicators = ['causa', 'produce', 'genera', 'resulta en', 'conduce a']
        # Moderate indicators
        moderate_indicators = ['permite', 'contribuye', 'facilita', 'mediante', 'a través de']
        # Weak indicators
        weak_indicators = ['con el fin de', 'para', 'porque']
        
        keyword_lower = keyword.lower()
        
        if any(ind in keyword_lower for ind in strong_indicators):
            return 0.90
        elif any(ind in keyword_lower for ind in moderate_indicators):
            return 0.70
        elif any(ind in keyword_lower for ind in weak_indicators):
            return 0.50
        
        return 0.60
    
    def _assess_temporal_coherence(self, source: str, target: str) -> float:
        """Assess temporal coherence based on verb sequences"""
        source_node = self.nodes.get(source)
        target_node = self.nodes.get(target)
        
        if not source_node or not target_node:
            return 0.5
        
        # Extract verbs from entity-activity if available
        if source_node.entity_activity and target_node.entity_activity:
            source_verb = source_node.entity_activity.verb_lemma
            target_verb = target_node.entity_activity.verb_lemma
            
            # Define logical verb sequences
            verb_sequences = {
                'diagnosticar': 1, 'planificar': 2, 'ejecutar': 3, 'evaluar': 4,
                'diseñar': 2, 'implementar': 3, 'monitorear': 4
            }
            
            source_seq = verb_sequences.get(source_verb, 5)
            target_seq = verb_sequences.get(target_verb, 5)
            
            if source_seq < target_seq:
                return 0.85
            elif source_seq == target_seq:
                return 0.60
            else:
                return 0.30
        
        return 0.50
    
    def _assess_financial_consistency(self, source: str, target: str) -> float:
        """Assess financial alignment between connected nodes"""
        source_node = self.nodes.get(source)
        target_node = self.nodes.get(target)
        
        if not source_node or not target_node:
            return 0.5
        
        source_budget = source_node.financial_allocation
        target_budget = target_node.financial_allocation
        
        if source_budget and target_budget:
            # Check if budgets are aligned (target should be <= source)
            ratio = target_budget / source_budget if source_budget > 0 else 0
            
            if 0.1 <= ratio <= 1.0:
                return 0.85
            elif ratio > 1.0 and ratio <= 1.5:
                return 0.60
            else:
                return 0.30
        
        return 0.50
    
    def _calculate_textual_proximity(self, source: str, target: str, text: str) -> float:
        """Calculate how often node IDs appear together in text windows"""
        window_size = 200  # characters
        co_occurrences = 0
        total_windows = 0
        
        source_positions = [m.start() for m in re.finditer(re.escape(source), text, re.IGNORECASE)]
        target_positions = [m.start() for m in re.finditer(re.escape(target), text, re.IGNORECASE)]
        
        for source_pos in source_positions:
            total_windows += 1
            for target_pos in target_positions:
                if abs(source_pos - target_pos) <= window_size:
                    co_occurrences += 1
                    break
        
        if total_windows > 0:
            proximity_score = co_occurrences / total_windows
            return proximity_score
        
        return 0.5
    
    def _initialize_prior(self, source: str, target: str) -> Tuple[float, float, float]:
        """Initialize prior distribution for causal link"""
        # Use type transition as base prior
        type_prior = self._calculate_type_transition_prior(source, target)
        
        # Beta distribution parameters - now externalized
        prior_alpha = self.config.get_bayesian_threshold('prior_alpha')
        prior_beta = self.config.get_bayesian_threshold('prior_beta')
        
        # Adjust based on type transition
        prior_mean = type_prior
        prior_strength = prior_alpha + prior_beta
        
        adjusted_alpha = prior_mean * prior_strength
        adjusted_beta = (1 - prior_mean) * prior_strength
        
        return prior_mean, adjusted_alpha, adjusted_beta
    
    def _calculate_composite_likelihood(self, evidence: Dict[str, Any]) -> float:
        """Calculate composite likelihood from multiple evidence components"""
        # Weight different evidence types
        weights = {
            'semantic_distance': 0.25,
            'type_transition_prior': 0.20,
            'language_specificity': 0.20,
            'temporal_coherence': 0.15,
            'financial_consistency': 0.10,
            'textual_proximity': 0.10
        }
        
        likelihood = 0.0
        for component, weight in weights.items():
            if component in evidence:
                likelihood += evidence[component] * weight
        
        return likelihood
    
    def _build_type_hierarchy(self) -> None:
        """Build hierarchy based on goal types"""
        type_order = {'programa': 0, 'producto': 1, 'resultado': 2, 'impacto': 3}
        
        nodes_by_type: Dict[str, List[str]] = defaultdict(list)
        for node_id in self.graph.nodes():
            node_type = self.graph.nodes[node_id].get('type', 'programa')
            nodes_by_type[node_type].append(node_id)
        
        # Connect productos to programas
        for prod in nodes_by_type.get('producto', []):
            for prog in nodes_by_type.get('programa', []):
                if not self.graph.has_edge(prog, prod):
                    self.graph.add_edge(prog, prod, logic='inferido', strength=0.5)
        
        # Connect resultados to productos
        for res in nodes_by_type.get('resultado', []):
            for prod in nodes_by_type.get('producto', []):
                if not self.graph.has_edge(prod, res):
                    self.graph.add_edge(prod, res, logic='inferido', strength=0.5)


class MechanismPartExtractor:
    """Extract Entity-Activity pairs for mechanism parts"""
    
    def __init__(self, config: ConfigLoader, nlp_model: spacy.Language) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.nlp = nlp_model
        self.entity_aliases = config.get('entity_aliases', {})
    
    def extract_entity_activity(self, text: str) -> Optional[EntityActivity]:
        """Extract Entity-Activity tuple from text"""
        doc = self.nlp(text)
        
        # Find main verb (activity)
        main_verb = None
        for token in doc:
            if token.pos_ == 'VERB' and token.dep_ in ['ROOT', 'ccomp']:
                main_verb = token
                break
        
        if not main_verb:
            return None
        
        # Find subject entity
        entity = None
        for child in main_verb.children:
            if child.dep_ in ['nsubj', 'nsubjpass']:
                entity = self._normalize_entity(child.text)
                break
        
        if not entity:
            # Try to find entity from NER
            for ent in doc.ents:
                if ent.label_ in ['ORG', 'PER']:
                    entity = self._normalize_entity(ent.text)
                    break
        
        if entity and main_verb:
            return EntityActivity(
                entity=entity,
                activity=main_verb.text,
                verb_lemma=main_verb.lemma_,
                confidence=0.85
            )
        
        return None
    
    def _normalize_entity(self, entity: str) -> str:
        """Normalize entity name using aliases"""
        entity_upper = entity.upper().strip()
        return self.entity_aliases.get(entity_upper, entity)


class FinancialAuditor:
    """Financial traceability and auditing"""
    
    def __init__(self, config: ConfigLoader) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.financial_data: Dict[str, Dict[str, float]] = {}
        self.unit_costs: Dict[str, float] = {}
        self.successful_parses = 0
        self.failed_parses = 0
    
    def trace_financial_allocation(self, tables: List[pd.DataFrame], 
                                  nodes: Dict[str, MetaNode]) -> Dict[str, float]:
        """Trace financial allocations to programs/goals"""
        for i, table in enumerate(tables):
            try:
                self.logger.info(f"Procesando tabla financiera {i+1}/{len(tables)}")
                self._process_financial_table(table, nodes)
                self.successful_parses += 1
            except Exception as e:
                self.logger.error(f"Error procesando tabla financiera {i+1}: {e}")
                self.failed_parses += 1
                continue
        
        self.logger.info(f"Asignaciones financieras trazadas: {len(self.financial_data)}")
        self.logger.info(f"Tablas parseadas exitosamente: {self.successful_parses}, "
                        f"Fallidas: {self.failed_parses}")
        return self.unit_costs
    
    def _process_financial_table(self, table: pd.DataFrame, 
                                nodes: Dict[str, MetaNode]) -> None:
        """Process a single financial table"""
        # Try to identify relevant columns
        amount_pattern = re.compile(
            self.config.get('patterns.financial_headers', r'PRESUPUESTO|VALOR|MONTO'),
            re.IGNORECASE
        )
        program_pattern = re.compile(r'PROGRAMA|META|CÓDIGO', re.IGNORECASE)
        
        amount_col = None
        program_col = None
        
        # Search in column names
        for col in table.columns:
            col_str = str(col)
            if amount_pattern.search(col_str) and not amount_col:
                amount_col = col
            if program_pattern.search(col_str) and not program_col:
                program_col = col
        
        # If not found in column names, search in first row
        if not amount_col or not program_col:
            first_row = table.iloc[0]
            for i, val in enumerate(first_row):
                val_str = str(val)
                if amount_pattern.search(val_str) and not amount_col:
                    amount_col = i
                    table.columns = table.iloc[0]
                    table = table[1:]
                if program_pattern.search(val_str) and not program_col:
                    program_col = i
                    table.columns = table.iloc[0]
                    table = table[1:]
        
        if amount_col is None or program_col is None:
            self.logger.warning("No se encontraron columnas financieras relevantes")
            return
        
        for _, row in table.iterrows():
            try:
                program_id = str(row[program_col]).strip().upper()
                amount = self._parse_amount(row[amount_col])
                
                if amount and program_id:
                    matched_node = self._match_program_to_node(program_id, nodes)
                    if matched_node:
                        self.financial_data[matched_node] = {
                            'allocation': amount,
                            'source': 'budget_table'
                        }
                        
                        # Update node
                        nodes[matched_node].financial_allocation = amount
                        
                        # Calculate unit cost if possible
                        node = nodes.get(matched_node)
                        if node and node.target:
                            try:
                                target_val = float(str(node.target).replace(',', '').replace('%', ''))
                                if target_val > 0:
                                    unit_cost = amount / target_val
                                    self.unit_costs[matched_node] = unit_cost
                                    nodes[matched_node].unit_cost = unit_cost
                            except (ValueError, TypeError):
                                pass
                                
            except Exception as e:
                self.logger.debug(f"Error procesando fila financiera: {e}")
                continue
    
    def _parse_amount(self, value: Any) -> Optional[float]:
        """Parse monetary amount from various formats"""
        if pd.isna(value):
            return None
        
        try:
            clean_value = str(value).replace('$', '').replace(',', '').replace(' ', '').replace('.', '')
            # Handle millions/thousands notation
            if 'M' in clean_value.upper() or 'MILLONES' in clean_value.upper():
                clean_value = clean_value.upper().replace('M', '').replace('ILLONES', '')
                return float(clean_value) * 1_000_000
            return float(clean_value)
        except (ValueError, TypeError):
            return None
    
    def _match_program_to_node(self, program_id: str, 
                               nodes: Dict[str, MetaNode]) -> Optional[str]:
        """Match program ID to existing node using fuzzy matching"""
        if program_id in nodes:
            return program_id
        
        # Try fuzzy matching
        best_match = process.extractOne(
            program_id,
            nodes.keys(),
            scorer=fuzz.ratio,
            score_cutoff=80
        )
        
        if best_match:
            return best_match[0]
        
        return None


class OperationalizationAuditor:
    """Audit operationalization quality"""
    
    def __init__(self, config: ConfigLoader) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.verb_sequences = config.get('verb_sequences', {})
        self.audit_results: Dict[str, AuditResult] = {}
        self.sequence_warnings: List[str] = []
    
    def audit_evidence_traceability(self, nodes: Dict[str, MetaNode]) -> Dict[str, AuditResult]:
        """Audit evidence traceability for all nodes"""
        for node_id, node in nodes.items():
            result: AuditResult = {
                'passed': True,
                'warnings': [],
                'errors': [],
                'recommendations': []
            }
            
            # Check baseline
            if not node.baseline or str(node.baseline).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                result['errors'].append(f"Línea base no definida para {node_id}")
                result['passed'] = False
                node.rigor_status = 'débil'
                node.audit_flags.append('sin_linea_base')
            
            # Check target
            if not node.target or str(node.target).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                result['errors'].append(f"Meta no definida para {node_id}")
                result['passed'] = False
                node.rigor_status = 'débil'
                node.audit_flags.append('sin_meta')
            
            # Check responsible entity
            if not node.responsible_entity:
                result['warnings'].append(f"Entidad responsable no identificada para {node_id}")
                node.audit_flags.append('sin_responsable')
            
            # Check financial traceability
            if not node.financial_allocation:
                result['warnings'].append(f"Sin trazabilidad financiera para {node_id}")
                node.audit_flags.append('sin_presupuesto')
            
            # Set rigor status if passed all checks
            if result['passed'] and len(result['warnings']) == 0:
                node.rigor_status = 'fuerte'
            
            self.audit_results[node_id] = result
        
        passed_count = sum(1 for r in self.audit_results.values() if r['passed'])
        self.logger.info(f"Auditoría de trazabilidad: {passed_count}/{len(nodes)} nodos aprobados")
        
        return self.audit_results
    
    def audit_sequence_logic(self, graph: nx.DiGraph) -> List[str]:
        """Audit logical sequence of activities"""
        warnings = []
        
        # Group nodes by program
        programs: Dict[str, List[str]] = defaultdict(list)
        for node_id in graph.nodes():
            node_data = graph.nodes[node_id]
            if node_data.get('type') == 'programa':
                for successor in graph.successors(node_id):
                    if graph.nodes[successor].get('type') == 'producto':
                        programs[node_id].append(successor)
        
        # Check sequence within each program
        for program_id, product_goals in programs.items():
            if len(product_goals) < 2:
                continue
            
            activities = []
            for goal_id in product_goals:
                node = graph.nodes[goal_id]
                ea = node.get('entity_activity')
                if ea and isinstance(ea, dict):
                    verb = ea.get('verb_lemma', '')
                    sequence_num = self.verb_sequences.get(verb, 999)
                    activities.append((goal_id, verb, sequence_num))
            
            # Check for sequence violations
            activities.sort(key=lambda x: x[2])
            for i in range(len(activities) - 1):
                if activities[i][2] > activities[i + 1][2]:
                    warning = (f"Violación de secuencia en {program_id}: "
                             f"{activities[i][1]} ({activities[i][0]}) "
                             f"antes de {activities[i + 1][1]} ({activities[i + 1][0]})")
                    warnings.append(warning)
                    self.logger.warning(warning)
        
        self.sequence_warnings = warnings
        return warnings
    
    def bayesian_counterfactual_audit(self, nodes: Dict[str, MetaNode], 
                                     graph: nx.DiGraph,
                                     historical_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        AGUJA III: El Auditor Contrafactual Bayesiano
        Perform counterfactual audit using Bayesian causal reasoning
        """
        self.logger.info("Iniciando auditoría contrafactual Bayesiana...")
        
        # Build implicit Structural Causal Model (SCM)
        scm_dag = self._build_normative_dag()
        
        # Initialize historical priors
        if historical_data is None:
            historical_data = self._get_default_historical_priors()
        
        # Audit results by layers
        layer1_results = self._audit_direct_evidence(nodes, scm_dag, historical_data)
        layer2_results = self._audit_causal_implications(nodes, graph, layer1_results)
        layer3_results = self._audit_systemic_risk(nodes, graph, layer1_results, layer2_results)
        
        # Generate optimal remediation recommendations
        recommendations = self._generate_optimal_remediations(
            layer1_results, layer2_results, layer3_results
        )
        
        audit_report = {
            'direct_evidence': layer1_results,
            'causal_implications': layer2_results,
            'systemic_risk': layer3_results,
            'recommendations': recommendations,
            'summary': {
                'total_nodes': len(nodes),
                'critical_omissions': sum(1 for r in layer1_results.values() 
                                        if r.get('omission_severity') == 'critical'),
                'expected_success_probability': layer3_results.get('success_probability', 0.0),
                'risk_score': layer3_results.get('risk_score', 0.0)
            }
        }
        
        self.logger.info(f"Auditoría contrafactual completada: "
                        f"{audit_report['summary']['critical_omissions']} omisiones críticas detectadas")
        
        return audit_report
    
    def _build_normative_dag(self) -> nx.DiGraph:
        """Build normative DAG of expected relationships in well-formed plans"""
        dag = nx.DiGraph()
        
        # Define normative structure
        # Each goal type should have these attributes
        dag.add_node('baseline', type='required_attribute')
        dag.add_node('target', type='required_attribute')
        dag.add_node('entity', type='required_attribute')
        dag.add_node('budget', type='recommended_attribute')
        dag.add_node('mechanism', type='recommended_attribute')
        dag.add_node('timeline', type='optional_attribute')
        dag.add_node('risk_factors', type='optional_attribute')
        
        # Causal relationships
        dag.add_edge('baseline', 'target', relation='defines_gap')
        dag.add_edge('entity', 'mechanism', relation='executes')
        dag.add_edge('budget', 'mechanism', relation='enables')
        dag.add_edge('mechanism', 'target', relation='achieves')
        dag.add_edge('risk_factors', 'target', relation='threatens')
        
        return dag
    
    def _get_default_historical_priors(self) -> Dict[str, Any]:
        """Get default historical priors if no data is available"""
        return {
            'entity_presence_success_rate': 0.94,
            'baseline_presence_success_rate': 0.89,
            'target_presence_success_rate': 0.92,
            'budget_presence_success_rate': 0.78,
            'mechanism_presence_success_rate': 0.65,
            'complete_documentation_success_rate': 0.82,
            'node_type_success_rates': {
                'producto': 0.85,
                'resultado': 0.72,
                'impacto': 0.58
            }
        }
    
    def _audit_direct_evidence(self, nodes: Dict[str, MetaNode],
                               scm_dag: nx.DiGraph,
                               historical_data: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """Layer 1: Audit direct evidence of required components"""
        results = {}
        
        for node_id, node in nodes.items():
            omissions = []
            omission_probs = {}
            
            # Check baseline
            if not node.baseline or str(node.baseline).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                p_failure_given_omission = 1.0 - historical_data.get('baseline_presence_success_rate', 0.89)
                omissions.append('baseline')
                omission_probs['baseline'] = p_failure_given_omission
            
            # Check target
            if not node.target or str(node.target).upper() in ['ND', 'POR DEFINIR', 'N/A', 'NONE']:
                p_failure_given_omission = 1.0 - historical_data.get('target_presence_success_rate', 0.92)
                omissions.append('target')
                omission_probs['target'] = p_failure_given_omission
            
            # Check entity
            if not node.responsible_entity:
                p_failure_given_omission = 1.0 - historical_data.get('entity_presence_success_rate', 0.94)
                omissions.append('entity')
                omission_probs['entity'] = p_failure_given_omission
            
            # Check budget
            if not node.financial_allocation:
                p_failure_given_omission = 1.0 - historical_data.get('budget_presence_success_rate', 0.78)
                omissions.append('budget')
                omission_probs['budget'] = p_failure_given_omission
            
            # Check mechanism
            if not node.entity_activity:
                p_failure_given_omission = 1.0 - historical_data.get('mechanism_presence_success_rate', 0.65)
                omissions.append('mechanism')
                omission_probs['mechanism'] = p_failure_given_omission
            
            # Determine severity
            severity = 'none'
            if omission_probs:
                max_failure_prob = max(omission_probs.values())
                if max_failure_prob > 0.15:
                    severity = 'critical'
                elif max_failure_prob > 0.10:
                    severity = 'high'
                elif max_failure_prob > 0.05:
                    severity = 'medium'
                else:
                    severity = 'low'
            
            results[node_id] = {
                'omissions': omissions,
                'omission_probabilities': omission_probs,
                'omission_severity': severity,
                'node_type': node.type
            }
        
        return results
    
    def _audit_causal_implications(self, nodes: Dict[str, MetaNode],
                                   graph: nx.DiGraph,
                                   direct_evidence: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """Layer 2: Audit causal implications of omissions"""
        implications = {}
        
        for node_id, node in nodes.items():
            node_omissions = direct_evidence[node_id]['omissions']
            causal_effects = {}
            
            # If baseline is missing
            if 'baseline' in node_omissions:
                # P(target_miscalibrated | missing_baseline)
                causal_effects['target_miscalibration'] = {
                    'probability': 0.73,
                    'description': 'Sin línea base, la meta probablemente está mal calibrada'
                }
            
            # If entity and high budget are missing
            if 'entity' in node_omissions and node.financial_allocation and node.financial_allocation > 1000000:
                causal_effects['implementation_failure'] = {
                    'probability': 0.89,
                    'description': 'Alto presupuesto sin entidad responsable indica alto riesgo de falla'
                }
            elif 'entity' in node_omissions:
                causal_effects['implementation_failure'] = {
                    'probability': 0.65,
                    'description': 'Sin entidad responsable, la implementación es incierta'
                }
            
            # If mechanism is missing
            if 'mechanism' in node_omissions:
                causal_effects['unclear_pathway'] = {
                    'probability': 0.70,
                    'description': 'Sin mecanismo definido, la vía causal es opaca'
                }
            
            # Check downstream effects
            successors = list(graph.successors(node_id)) if graph.has_node(node_id) else []
            if node_omissions and successors:
                causal_effects['cascade_risk'] = {
                    'probability': min(0.95, 0.4 + 0.1 * len(node_omissions)),
                    'affected_nodes': successors,
                    'description': f'Omisiones pueden afectar {len(successors)} nodos dependientes'
                }
            
            implications[node_id] = {
                'causal_effects': causal_effects,
                'total_risk': sum(e['probability'] for e in causal_effects.values()) / max(len(causal_effects), 1)
            }
        
        return implications
    
    def _audit_systemic_risk(self, nodes: Dict[str, MetaNode],
                            graph: nx.DiGraph,
                            direct_evidence: Dict[str, Dict[str, Any]],
                            causal_implications: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """Layer 3: Calculate systemic risk from accumulated omissions"""
        
        # Identify critical nodes (high centrality)
        if graph.number_of_nodes() > 0:
            try:
                centrality = nx.betweenness_centrality(graph)
            except:
                centrality = {n: 0.5 for n in graph.nodes()}
        else:
            centrality = {}
        
        # Calculate P(cascade_failure | omission_set)
        critical_omissions = []
        for node_id, evidence in direct_evidence.items():
            if evidence['omission_severity'] in ['critical', 'high']:
                node_centrality = centrality.get(node_id, 0.5)
                critical_omissions.append({
                    'node_id': node_id,
                    'severity': evidence['omission_severity'],
                    'centrality': node_centrality,
                    'omissions': evidence['omissions']
                })
        
        # Calculate systemic risk
        if critical_omissions:
            # Weighted by centrality
            risk_score = sum(
                (1.0 if om['severity'] == 'critical' else 0.7) * (om['centrality'] + 0.1)
                for om in critical_omissions
            ) / len(nodes)
        else:
            risk_score = 0.0
        
        # Calculate P(success | current_state)
        total_omissions = sum(len(e['omissions']) for e in direct_evidence.values())
        total_possible = len(nodes) * 5  # 5 key attributes per node
        completeness = 1.0 - (total_omissions / max(total_possible, 1))
        
        # Success probability (simplified Bayesian)
        base_success_rate = 0.70
        success_probability = base_success_rate * completeness
        
        return {
            'risk_score': min(1.0, risk_score),
            'success_probability': success_probability,
            'critical_omissions': critical_omissions,
            'completeness': completeness,
            'total_omissions': total_omissions
        }
    
    def _generate_optimal_remediations(self, 
                                      direct_evidence: Dict[str, Dict[str, Any]],
                                      causal_implications: Dict[str, Dict[str, Any]],
                                      systemic_risk: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate prioritized remediation recommendations"""
        remediations = []
        
        # Calculate expected value of information for each remediation
        for node_id, evidence in direct_evidence.items():
            if not evidence['omissions']:
                continue
            
            for omission in evidence['omissions']:
                # Estimate impact
                omission_prob = evidence['omission_probabilities'].get(omission, 0.1)
                causal_risk = causal_implications[node_id]['total_risk']
                
                # Expected value = P(failure_avoided) * Impact
                expected_value = omission_prob * (1 + causal_risk)
                
                # Effort estimate (simplified)
                effort_map = {
                    'baseline': 3,  # Moderate effort to research
                    'target': 2,    # Low effort to define
                    'entity': 2,    # Low effort to assign
                    'budget': 4,    # Higher effort to allocate
                    'mechanism': 5  # Highest effort to design
                }
                effort = effort_map.get(omission, 3)
                
                # Priority = Expected Value / Effort
                priority = expected_value / effort
                
                remediations.append({
                    'node_id': node_id,
                    'omission': omission,
                    'severity': evidence['omission_severity'],
                    'expected_value': expected_value,
                    'effort': effort,
                    'priority': priority,
                    'recommendation': self._get_remediation_text(omission, node_id)
                })
        
        # Sort by priority (descending)
        remediations.sort(key=lambda x: x['priority'], reverse=True)
        
        return remediations
    
    def _get_remediation_text(self, omission: str, node_id: str) -> str:
        """Get specific remediation text for an omission"""
        texts = {
            'baseline': f"Definir línea base cuantitativa para {node_id} basada en diagnóstico actual",
            'target': f"Especificar meta cuantitativa alcanzable para {node_id} con horizonte temporal",
            'entity': f"Asignar entidad responsable clara para la ejecución de {node_id}",
            'budget': f"Asignar recursos presupuestarios específicos a {node_id}",
            'mechanism': f"Documentar mecanismo causal (Entidad-Actividad) para {node_id}"
        }
        return texts.get(omission, f"Completar {omission} para {node_id}")



class BayesianMechanismInference:
    """
    AGUJA II: El Modelo Generativo de Mecanismos
    Hierarchical Bayesian model for causal mechanism inference
    """
    
    def __init__(self, config: ConfigLoader, nlp_model: spacy.Language) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.nlp = nlp_model
        
        # Load mechanism type hyperpriors from configuration (externalized)
        self.mechanism_type_priors = {
            'administrativo': self.config.get_mechanism_prior('administrativo'),
            'tecnico': self.config.get_mechanism_prior('tecnico'),
            'financiero': self.config.get_mechanism_prior('financiero'),
            'politico': self.config.get_mechanism_prior('politico'),
            'mixto': self.config.get_mechanism_prior('mixto')
        }
        
        # Typical activity sequences by mechanism type
        # These could also be externalized if needed for domain-specific customization
        self.mechanism_sequences = {
            'administrativo': ['planificar', 'coordinar', 'gestionar', 'supervisar'],
            'tecnico': ['diagnosticar', 'diseñar', 'implementar', 'evaluar'],
            'financiero': ['asignar', 'ejecutar', 'auditar', 'reportar'],
            'politico': ['concertar', 'negociar', 'aprobar', 'promulgar']
        }
        
        # Track inferred mechanisms
        self.inferred_mechanisms: Dict[str, Dict[str, Any]] = {}
    
    def infer_mechanisms(self, nodes: Dict[str, MetaNode], 
                        text: str) -> Dict[str, Dict[str, Any]]:
        """
        Infer latent causal mechanisms using hierarchical Bayesian modeling
        """
        self.logger.info("Iniciando inferencia Bayesiana de mecanismos...")
        
        # Focus on 'producto' nodes which should have mechanisms
        product_nodes = {nid: n for nid, n in nodes.items() if n.type == 'producto'}
        
        for node_id, node in product_nodes.items():
            mechanism = self._infer_single_mechanism(node, text, nodes)
            self.inferred_mechanisms[node_id] = mechanism
        
        self.logger.info(f"Mecanismos inferidos: {len(self.inferred_mechanisms)}")
        return self.inferred_mechanisms
    
    def _infer_single_mechanism(self, node: MetaNode, text: str, 
                                all_nodes: Dict[str, MetaNode]) -> Dict[str, Any]:
        """Infer mechanism for a single product node"""
        # Extract observations from text
        observations = self._extract_observations(node, text)
        
        # Level 3: Sample mechanism type from hyperprior
        mechanism_type_posterior = self._infer_mechanism_type(observations)
        
        # Level 2: Infer activity sequence given mechanism type
        sequence_posterior = self._infer_activity_sequence(
            observations, mechanism_type_posterior
        )
        
        # Level 1: Calculate coherence factor
        coherence_score = self._calculate_coherence_factor(
            node, observations, all_nodes
        )
        
        # Validation tests
        sufficiency = self._test_sufficiency(node, observations)
        necessity = self._test_necessity(node, observations)
        
        # Quantify uncertainty
        uncertainty = self._quantify_uncertainty(
            mechanism_type_posterior, sequence_posterior, coherence_score
        )
        
        # Detect gaps
        gaps = self._detect_gaps(node, observations, uncertainty)
        
        return {
            'mechanism_type': mechanism_type_posterior,
            'activity_sequence': sequence_posterior,
            'coherence_score': coherence_score,
            'sufficiency_test': sufficiency,
            'necessity_test': necessity,
            'uncertainty': uncertainty,
            'gaps': gaps,
            'observations': observations
        }
    
    def _extract_observations(self, node: MetaNode, text: str) -> Dict[str, Any]:
        """Extract textual observations related to the mechanism"""
        # Find node context in text
        node_pattern = re.escape(node.id)
        matches = list(re.finditer(node_pattern, text, re.IGNORECASE))
        
        observations = {
            'entity_activity': None,
            'verbs': [],
            'entities': [],
            'budget': node.financial_allocation,
            'context_snippets': []
        }
        
        if node.entity_activity:
            observations['entity_activity'] = {
                'entity': node.entity_activity.entity,
                'activity': node.entity_activity.activity,
                'verb_lemma': node.entity_activity.verb_lemma
            }
        
        # Extract context around node mentions
        for match in matches[:3]:  # Limit to first 3 occurrences
            start = max(0, match.start() - 300)
            end = min(len(text), match.end() + 300)
            context = text[start:end]
            
            # Process with spaCy
            doc = self.nlp(context)
            
            # Extract verbs
            verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']
            observations['verbs'].extend(verbs)
            
            # Extract entities
            entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PER']]
            observations['entities'].extend(entities)
            
            observations['context_snippets'].append(context[:200])
        
        return observations
    
    def _infer_mechanism_type(self, observations: Dict[str, Any]) -> Dict[str, float]:
        """Infer mechanism type using Bayesian updating"""
        # Start with hyperprior
        posterior = dict(self.mechanism_type_priors)
        
        # Get Laplace smoothing parameter from configuration
        laplace_smooth = self.config.get_bayesian_threshold('laplace_smoothing')
        
        # Update based on observed verbs
        observed_verbs = set(observations.get('verbs', []))
        
        if observed_verbs:
            for mech_type, typical_verbs in self.mechanism_sequences.items():
                # Count overlap
                overlap = len(observed_verbs.intersection(set(typical_verbs)))
                total = len(typical_verbs)
                
                if total > 0:
                    # Likelihood: proportion of typical verbs observed with Laplace smoothing
                    likelihood = (overlap + laplace_smooth) / (total + 2 * laplace_smooth)
                    
                    # Bayesian update
                    posterior[mech_type] *= likelihood
        
        # Update based on entity-activity
        if observations.get('entity_activity'):
            verb = observations['entity_activity'].get('verb_lemma', '')
            for mech_type, typical_verbs in self.mechanism_sequences.items():
                if verb in typical_verbs:
                    posterior[mech_type] *= 1.5
        
        # Normalize
        total = sum(posterior.values())
        if total > 0:
            posterior = {k: v / total for k, v in posterior.items()}
        
        return posterior
    
    def _infer_activity_sequence(self, observations: Dict[str, Any],
                                 mechanism_type_posterior: Dict[str, float]) -> Dict[str, Any]:
        """Infer activity sequence parameters"""
        # Get most likely mechanism type
        best_type = max(mechanism_type_posterior.items(), key=lambda x: x[1])[0]
        expected_sequence = self.mechanism_sequences.get(best_type, [])
        
        observed_verbs = observations.get('verbs', [])
        
        # Calculate transition probabilities (simplified Markov chain)
        transitions = {}
        for i in range(len(expected_sequence) - 1):
            current = expected_sequence[i]
            next_verb = expected_sequence[i + 1]
            
            # Check if transition is observed
            if current in observed_verbs and next_verb in observed_verbs:
                transitions[(current, next_verb)] = 0.85
            else:
                transitions[(current, next_verb)] = 0.40
        
        return {
            'expected_sequence': expected_sequence,
            'observed_verbs': observed_verbs,
            'transition_probabilities': transitions,
            'sequence_completeness': len(set(observed_verbs) & set(expected_sequence)) / max(len(expected_sequence), 1)
        }
    
    def _calculate_coherence_factor(self, node: MetaNode, 
                                   observations: Dict[str, Any],
                                   all_nodes: Dict[str, MetaNode]) -> float:
        """Calculate mechanism coherence score"""
        coherence = 0.0
        weights = []
        
        # Factor 1: Entity-Activity presence
        if observations.get('entity_activity'):
            coherence += 0.30
            weights.append(0.30)
        
        # Factor 2: Budget consistency
        if observations.get('budget'):
            coherence += 0.20
            weights.append(0.20)
        
        # Factor 3: Verb sequence completeness
        seq_info = observations.get('verbs', [])
        if seq_info:
            verb_score = min(len(seq_info) / 4.0, 1.0)  # Expect ~4 verbs
            coherence += verb_score * 0.25
            weights.append(0.25)
        
        # Factor 4: Entity presence
        if observations.get('entities'):
            coherence += 0.15
            weights.append(0.15)
        
        # Factor 5: Context richness
        snippets = observations.get('context_snippets', [])
        if snippets:
            coherence += 0.10
            weights.append(0.10)
        
        # Normalize by actual weights used
        if weights:
            coherence = coherence / sum(weights) if sum(weights) > 0 else 0.0
        
        return coherence
    
    def _test_sufficiency(self, node: MetaNode, 
                         observations: Dict[str, Any]) -> Dict[str, Any]:
        """Test if mechanism is sufficient to produce the outcome"""
        # Check if entity has capability
        has_entity = observations.get('entity_activity') is not None
        
        # Check if activities are present
        has_activities = len(observations.get('verbs', [])) >= 2
        
        # Check if resources are allocated
        has_resources = observations.get('budget') is not None
        
        sufficiency_score = (
            (0.4 if has_entity else 0.0) +
            (0.4 if has_activities else 0.0) +
            (0.2 if has_resources else 0.0)
        )
        
        return {
            'score': sufficiency_score,
            'is_sufficient': sufficiency_score >= 0.6,
            'components': {
                'entity': has_entity,
                'activities': has_activities,
                'resources': has_resources
            }
        }
    
    def _test_necessity(self, node: MetaNode, 
                       observations: Dict[str, Any]) -> Dict[str, Any]:
        """Test if mechanism is necessary or if alternatives exist"""
        # Check uniqueness of entity
        entities = observations.get('entities', [])
        unique_entity = len(set(entities)) == 1 if entities else False
        
        # Check specificity of activities
        verbs = observations.get('verbs', [])
        specific_verbs = len([v for v in verbs if v in [
            'implementar', 'ejecutar', 'realizar', 'desarrollar'
        ]]) > 0
        
        necessity_score = (
            (0.5 if unique_entity else 0.3) +
            (0.5 if specific_verbs else 0.3)
        )
        
        return {
            'score': necessity_score,
            'is_necessary': necessity_score >= 0.7,
            'alternatives_likely': necessity_score < 0.5
        }
    
    def _quantify_uncertainty(self, mechanism_type_posterior: Dict[str, float],
                             sequence_posterior: Dict[str, Any],
                             coherence_score: float) -> Dict[str, float]:
        """Quantify epistemic uncertainty"""
        # Entropy of mechanism type distribution
        mech_probs = list(mechanism_type_posterior.values())
        if mech_probs:
            mech_entropy = -sum(p * np.log(p + 1e-10) for p in mech_probs if p > 0)
            max_entropy = np.log(len(mech_probs))
            mech_uncertainty = mech_entropy / max_entropy if max_entropy > 0 else 1.0
        else:
            mech_uncertainty = 1.0
        
        # Sequence completeness uncertainty
        seq_completeness = sequence_posterior.get('sequence_completeness', 0.0)
        seq_uncertainty = 1.0 - seq_completeness
        
        # Coherence uncertainty
        coherence_uncertainty = 1.0 - coherence_score
        
        # Combined uncertainty
        total_uncertainty = (
            mech_uncertainty * 0.4 +
            seq_uncertainty * 0.3 +
            coherence_uncertainty * 0.3
        )
        
        return {
            'total': total_uncertainty,
            'mechanism_type': mech_uncertainty,
            'sequence': seq_uncertainty,
            'coherence': coherence_uncertainty
        }
    
    def _detect_gaps(self, node: MetaNode, observations: Dict[str, Any],
                    uncertainty: Dict[str, float]) -> List[Dict[str, str]]:
        """Detect documentation gaps based on uncertainty"""
        gaps = []
        
        # High total uncertainty
        if uncertainty['total'] > 0.6:
            gaps.append({
                'type': 'high_uncertainty',
                'severity': 'high',
                'message': f"Mecanismo para {node.id} tiene alta incertidumbre ({uncertainty['total']:.2f})",
                'suggestion': "Se requiere más documentación sobre el mecanismo causal"
            })
        
        # Missing entity
        if not observations.get('entity_activity'):
            gaps.append({
                'type': 'missing_entity',
                'severity': 'high',
                'message': f"No se especifica entidad responsable para {node.id}",
                'suggestion': "Especificar qué entidad ejecutará las actividades"
            })
        
        # Insufficient activities
        if len(observations.get('verbs', [])) < 2:
            gaps.append({
                'type': 'insufficient_activities',
                'severity': 'medium',
                'message': f"Pocas actividades documentadas para {node.id}",
                'suggestion': "Detallar las actividades necesarias para lograr el producto"
            })
        
        # Missing budget
        if not observations.get('budget'):
            gaps.append({
                'type': 'missing_budget',
                'severity': 'medium',
                'message': f"Sin asignación presupuestaria para {node.id}",
                'suggestion': "Asignar recursos financieros al producto"
            })
        
        return gaps


class CausalInferenceSetup:
    """Prepare model for causal inference"""
    
    def __init__(self, config: ConfigLoader) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.goal_classification = config.get('lexicons.goal_classification', {})
        self.admin_keywords = config.get('lexicons.administrative_keywords', [])
        self.contextual_factors = config.get('lexicons.contextual_factors', [])
    
    def classify_goal_dynamics(self, nodes: Dict[str, MetaNode]) -> None:
        """Classify dynamics for each goal"""
        for node in nodes.values():
            text_lower = node.text.lower()
            
            for keyword, dynamics in self.goal_classification.items():
                if keyword in text_lower:
                    node.dynamics = cast(DynamicsType, dynamics)
                    self.logger.debug(f"Meta {node.id} clasificada como {node.dynamics}")
                    break
    
    def assign_probative_value(self, nodes: Dict[str, MetaNode]) -> None:
        """Assign probative test types to nodes"""
        for node in nodes.values():
            text_lower = node.text.lower()
            
            # Check for administrative/regulatory nature (Hoop Test)
            if any(keyword in text_lower for keyword in self.admin_keywords):
                node.test_type = 'hoop_test'
            # Check for highly specific outcomes (Smoking Gun)
            elif node.type == 'resultado' and node.target and node.baseline:
                try:
                    float(str(node.target).replace(',', '').replace('%', ''))
                    node.test_type = 'smoking_gun'
                except (ValueError, TypeError):
                    node.test_type = 'straw_in_wind'
            # Double decisive for critical impact goals
            elif node.type == 'impacto' and node.rigor_status == 'fuerte':
                node.test_type = 'doubly_decisive'
            else:
                node.test_type = 'straw_in_wind'
            
            self.logger.debug(f"Meta {node.id} asignada test type: {node.test_type}")
    
    def identify_failure_points(self, graph: nx.DiGraph, text: str) -> Set[str]:
        """Identify single points of failure in causal chain"""
        failure_points = set()
        
        # Find nodes with high out-degree (many dependencies)
        for node_id in graph.nodes():
            out_degree = graph.out_degree(node_id)
            node_type = graph.nodes[node_id].get('type')
            
            if node_type == 'producto' and out_degree >= 3:
                failure_points.add(node_id)
                self.logger.warning(f"Punto único de falla identificado: {node_id} "
                                  f"(grado de salida: {out_degree})")
        
        # Extract contextual risks from text
        risk_pattern = '|'.join(re.escape(factor) for factor in self.contextual_factors)
        risk_regex = re.compile(rf'\b({risk_pattern})\b', re.IGNORECASE)
        
        # Find risk mentions and associate with nodes
        for match in risk_regex.finditer(text):
            risk_text = match.group()
            context_start = max(0, match.start() - 200)
            context_end = min(len(text), match.end() + 200)
            context = text[context_start:context_end]
            
            # Try to find node mentions in risk context
            for node_id in graph.nodes():
                if node_id in context:
                    failure_points.add(node_id)
                    if 'contextual_risks' not in graph.nodes[node_id]:
                        graph.nodes[node_id]['contextual_risks'] = []
                    graph.nodes[node_id]['contextual_risks'].append(risk_text)
        
        self.logger.info(f"Puntos de falla identificados: {len(failure_points)}")
        return failure_points


class ReportingEngine:
    """Generate visualizations and reports"""
    
    def __init__(self, config: ConfigLoader, output_dir: Path) -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.config = config
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_causal_diagram(self, graph: nx.DiGraph, policy_code: str) -> Path:
        """Generate causal diagram visualization"""
        dot = Dot(graph_type='digraph', rankdir='TB')
        dot.set_name(f'{policy_code}_causal_model')
        dot.set_node_defaults(
            shape='box',
            style='rounded,filled',
            fontname='Arial',
            fontsize='10'
        )
        dot.set_edge_defaults(
            fontsize='8',
            fontname='Arial'
        )
        
        # Add nodes with rigor coloring
        for node_id in graph.nodes():
            node_data = graph.nodes[node_id]
            
            # Determine color based on rigor status and audit flags
            rigor = node_data.get('rigor_status', 'sin_evaluar')
            audit_flags = node_data.get('audit_flags', [])
            financial = node_data.get('financial_allocation')
            
            if rigor == 'débil' or not financial:
                color = 'lightcoral'  # Red
            elif audit_flags:
                color = 'lightyellow'  # Yellow
            else:
                color = 'lightgreen'  # Green
            
            # Create label
            node_type = node_data.get('type', 'programa')
            text = node_data.get('text', '')[:80]
            label = f"{node_id}\\n[{node_type.upper()}]\\n{text}..."
            
            entity = node_data.get('responsible_entity')
            if entity:
                label += f"\\n👤 {entity[:30]}"
            
            if financial:
                label += f"\\n💰 ${financial:,.0f}"
            
            dot_node = Node(
                node_id,
                label=label,
                fillcolor=color
            )
            dot.add_node(dot_node)
        
        # Add edges with causal logic
        for source, target in graph.edges():
            edge_data = graph.edges[source, target]
            keyword = edge_data.get('keyword', '')
            strength = edge_data.get('strength', 0.5)
            
            # Determine edge style based on strength
            style = 'solid' if strength > 0.7 else 'dashed'
            
            dot_edge = Edge(
                source,
                target,
                label=keyword[:20],
                style=style
            )
            dot.add_edge(dot_edge)
        
        # Save files
        dot_path = self.output_dir / f"{policy_code}_causal_diagram.dot"
        png_path = self.output_dir / f"{policy_code}_causal_diagram.png"
        
        try:
            with open(dot_path, 'w', encoding='utf-8') as f:
                f.write(dot.to_string())
            self.logger.info(f"Diagrama DOT guardado en: {dot_path}")
            
            # Try to render PNG
            try:
                dot.write_png(str(png_path))
                self.logger.info(f"Diagrama PNG renderizado en: {png_path}")
            except Exception as e:
                self.logger.warning(f"No se pudo renderizar PNG (¿Graphviz instalado?): {e}")
        except Exception as e:
            self.logger.error(f"Error guardando diagrama: {e}")
        
        return png_path
    
    def generate_accountability_matrix(self, graph: nx.DiGraph, 
                                      policy_code: str) -> Path:
        """Generate accountability matrix in Markdown"""
        md_path = self.output_dir / f"{policy_code}_accountability_matrix.md"
        
        # Group by impact goals
        impact_goals = [n for n in graph.nodes() 
                       if graph.nodes[n].get('type') == 'impacto']
        
        content = [f"# Matriz de Responsabilidades - {policy_code}\n"]
        content.append(f"*Generado automáticamente por CDAF v2.0*\n")
        content.append("---\n\n")
        
        for impact in impact_goals:
            impact_data = graph.nodes[impact]
            content.append(f"## Meta de Impacto: {impact}\n")
            content.append(f"**Descripción:** {impact_data.get('text', 'N/A')}\n\n")
            
            # Find all predecessor chains
            predecessors = list(nx.ancestors(graph, impact))
            
            if predecessors:
                content.append("| Meta | Tipo | Entidad Responsable | Actividad Clave | Presupuesto |\n")
                content.append("|------|------|---------------------|-----------------|-------------|\n")
                
                for pred in predecessors:
                    pred_data = graph.nodes[pred]
                    meta_type = pred_data.get('type', 'N/A')
                    entity = pred_data.get('responsible_entity', 'No asignado')
                    
                    ea = pred_data.get('entity_activity')
                    activity = 'N/A'
                    if ea and isinstance(ea, dict):
                        activity = ea.get('activity', 'N/A')
                    
                    budget = pred_data.get('financial_allocation')
                    budget_str = f"${budget:,.0f}" if budget else "Sin presupuesto"
                    
                    content.append(f"| {pred} | {meta_type} | {entity} | {activity} | {budget_str} |\n")
                
                content.append("\n")
            else:
                content.append("*No se encontraron metas intermedias.*\n\n")
        
        content.append("\n---\n")
        content.append("### Leyenda\n")
        content.append("- **Meta de Impacto:** Resultado final esperado\n")
        content.append("- **Meta de Resultado:** Cambio intermedio observable\n")
        content.append("- **Meta de Producto:** Entrega tangible del programa\n")
        
        try:
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(''.join(content))
            self.logger.info(f"Matriz de responsabilidades guardada en: {md_path}")
        except Exception as e:
            self.logger.error(f"Error guardando matriz de responsabilidades: {e}")
        
        return md_path
    
    def generate_confidence_report(self, 
                                   nodes: Dict[str, MetaNode],
                                   graph: nx.DiGraph,
                                   causal_chains: List[CausalLink],
                                   audit_results: Dict[str, AuditResult],
                                   financial_auditor: FinancialAuditor,
                                   sequence_warnings: List[str],
                                   policy_code: str) -> Path:
        """Generate extraction confidence report"""
        json_path = self.output_dir / f"{policy_code}_extraction_confidence_report.json"
        
        # Calculate metrics
        total_metas = len(nodes)
        
        metas_with_ea = sum(1 for n in nodes.values() if n.entity_activity)
        metas_with_ea_pct = (metas_with_ea / total_metas * 100) if total_metas > 0 else 0
        
        enlaces_with_logic = sum(1 for link in causal_chains if link.get('logic'))
        total_edges = graph.number_of_edges()
        enlaces_with_logic_pct = (enlaces_with_logic / total_edges * 100) if total_edges > 0 else 0
        
        metas_passed_audit = sum(1 for r in audit_results.values() if r['passed'])
        metas_with_traceability_pct = (metas_passed_audit / total_metas * 100) if total_metas > 0 else 0
        
        metas_with_financial = sum(1 for n in nodes.values() if n.financial_allocation)
        metas_with_financial_pct = (metas_with_financial / total_metas * 100) if total_metas > 0 else 0
        
        # Node type distribution
        type_distribution = defaultdict(int)
        for node in nodes.values():
            type_distribution[node.type] += 1
        
        # Rigor distribution
        rigor_distribution = defaultdict(int)
        for node in nodes.values():
            rigor_distribution[node.rigor_status] += 1
        
        report = {
            "metadata": {
                "policy_code": policy_code,
                "framework_version": "2.0.0",
                "total_nodes": total_metas,
                "total_edges": total_edges
            },
            "extraction_metrics": {
                "total_metas_identificadas": total_metas,
                "metas_con_EA_extraido": metas_with_ea,
                "metas_con_EA_extraido_pct": round(metas_with_ea_pct, 2),
                "enlaces_con_logica_causal": enlaces_with_logic,
                "enlaces_con_logica_causal_pct": round(enlaces_with_logic_pct, 2),
                "metas_con_trazabilidad_evidencia": metas_passed_audit,
                "metas_con_trazabilidad_evidencia_pct": round(metas_with_traceability_pct, 2),
                "metas_con_trazabilidad_financiera": metas_with_financial,
                "metas_con_trazabilidad_financiera_pct": round(metas_with_financial_pct, 2)
            },
            "financial_audit": {
                "tablas_financieras_parseadas_exitosamente": financial_auditor.successful_parses,
                "tablas_financieras_fallidas": financial_auditor.failed_parses,
                "asignaciones_presupuestarias_rastreadas": len(financial_auditor.financial_data)
            },
            "sequence_audit": {
                "alertas_secuencia_logica": len(sequence_warnings),
                "detalles": sequence_warnings
            },
            "type_distribution": dict(type_distribution),
            "rigor_distribution": dict(rigor_distribution),
            "audit_summary": {
                "total_audited": len(audit_results),
                "passed": sum(1 for r in audit_results.values() if r['passed']),
                "failed": sum(1 for r in audit_results.values() if not r['passed']),
                "total_warnings": sum(len(r['warnings']) for r in audit_results.values()),
                "total_errors": sum(len(r['errors']) for r in audit_results.values())
            },
            "quality_score": self._calculate_quality_score(
                metas_with_traceability_pct,
                metas_with_financial_pct,
                enlaces_with_logic_pct,
                metas_with_ea_pct
            )
        }
        
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Reporte de confianza guardado en: {json_path}")
        except Exception as e:
            self.logger.error(f"Error guardando reporte de confianza: {e}")
        
        return json_path
    
    def _calculate_quality_score(self, traceability: float, financial: float, 
                                 logic: float, ea: float) -> float:
        """Calculate overall quality score (0-100)"""
        weights = {'traceability': 0.35, 'financial': 0.25, 'logic': 0.25, 'ea': 0.15}
        score = (traceability * weights['traceability'] +
                financial * weights['financial'] +
                logic * weights['logic'] +
                ea * weights['ea'])
        return round(score, 2)
    
    def generate_causal_model_json(self, graph: nx.DiGraph, nodes: Dict[str, MetaNode],
                                  policy_code: str) -> Path:
        """Generate structured JSON export of causal model"""
        json_path = self.output_dir / f"{policy_code}_causal_model.json"
        
        # Prepare node data
        nodes_data = {}
        for node_id, node in nodes.items():
            node_dict = asdict(node)
            # Convert NamedTuple to dict
            if node.entity_activity:
                node_dict['entity_activity'] = node.entity_activity._asdict()
            nodes_data[node_id] = node_dict
        
        # Prepare edge data
        edges_data = []
        for source, target in graph.edges():
            edge_dict = {
                'source': source,
                'target': target,
                **graph.edges[source, target]
            }
            edges_data.append(edge_dict)
        
        model_data = {
            "policy_code": policy_code,
            "framework_version": "2.0.0",
            "nodes": nodes_data,
            "edges": edges_data,
            "statistics": {
                "total_nodes": len(nodes_data),
                "total_edges": len(edges_data),
                "node_types": {
                    node_type: sum(1 for n in nodes.values() if n.type == node_type)
                    for node_type in ['programa', 'producto', 'resultado', 'impacto']
                }
            }
        }
        
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(model_data, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Modelo causal JSON guardado en: {json_path}")
        except Exception as e:
            self.logger.error(f"Error guardando modelo causal: {e}")
        
        return json_path


class CDAFFramework:
    """Main orchestrator for the CDAF pipeline"""
    
    def __init__(self, config_path: Path, output_dir: Path, log_level: str = "INFO") -> None:
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.setLevel(getattr(logging, log_level.upper()))
        
        # Initialize components
        self.config = ConfigLoader(config_path)
        self.output_dir = output_dir
        
        # Load spaCy model
        try:
            self.nlp = spacy.load("es_core_news_lg")
            self.logger.info("Modelo spaCy cargado: es_core_news_lg")
        except OSError:
            self.logger.warning("Modelo es_core_news_lg no encontrado. Intentando es_core_news_sm...")
            try:
                self.nlp = spacy.load("es_core_news_sm")
            except OSError:
                self.logger.error("No se encontró ningún modelo de spaCy en español. "
                                "Ejecute: python -m spacy download es_core_news_lg")
                sys.exit(1)
        
        # Initialize modules
        self.pdf_processor = PDFProcessor(self.config)
        self.causal_extractor = CausalExtractor(self.config, self.nlp)
        self.mechanism_extractor = MechanismPartExtractor(self.config, self.nlp)
        self.bayesian_mechanism = BayesianMechanismInference(self.config, self.nlp)
        self.financial_auditor = FinancialAuditor(self.config)
        self.op_auditor = OperationalizationAuditor(self.config)
        self.inference_setup = CausalInferenceSetup(self.config)
        self.reporting_engine = ReportingEngine(self.config, output_dir)
        
        # Initialize DNP validator if available
        self.dnp_validator = None
        if DNP_AVAILABLE:
            self.dnp_validator = ValidadorDNP(es_municipio_pdet=False)  # Can be configured
            self.logger.info("Validador DNP inicializado")
    
    def process_document(self, pdf_path: Path, policy_code: str) -> bool:
        """Main processing pipeline"""
        self.logger.info(f"Iniciando procesamiento de documento: {pdf_path}")
        
        try:
            # Step 1: Load and extract PDF
            if not self.pdf_processor.load_document(pdf_path):
                return False
            
            text = self.pdf_processor.extract_text()
            tables = self.pdf_processor.extract_tables()
            sections = self.pdf_processor.extract_sections()
            
            # Step 2: Extract causal hierarchy
            self.logger.info("Extrayendo jerarquía causal...")
            graph = self.causal_extractor.extract_causal_hierarchy(text)
            nodes = self.causal_extractor.nodes
            
            # Step 3: Extract Entity-Activity pairs
            self.logger.info("Extrayendo tuplas Entidad-Actividad...")
            for node in nodes.values():
                if node.type == 'producto':
                    ea = self.mechanism_extractor.extract_entity_activity(node.text)
                    if ea:
                        node.entity_activity = ea
                        graph.nodes[node.id]['entity_activity'] = ea._asdict()
            
            # Step 4: Financial traceability
            self.logger.info("Auditando trazabilidad financiera...")
            self.financial_auditor.trace_financial_allocation(tables, nodes)
            
            # Step 4.5: Bayesian Mechanism Inference (AGUJA II)
            self.logger.info("Infiriendo mecanismos causales con modelo Bayesiano...")
            inferred_mechanisms = self.bayesian_mechanism.infer_mechanisms(nodes, text)
            
            # Step 5: Operationalization audit
            self.logger.info("Auditando operacionalización...")
            audit_results = self.op_auditor.audit_evidence_traceability(nodes)
            sequence_warnings = self.op_auditor.audit_sequence_logic(graph)
            
            # Step 5.5: Bayesian Counterfactual Audit (AGUJA III)
            self.logger.info("Ejecutando auditoría contrafactual Bayesiana...")
            counterfactual_audit = self.op_auditor.bayesian_counterfactual_audit(nodes, graph)
            
            # Step 6: Causal inference setup
            self.logger.info("Preparando para inferencia causal...")
            self.inference_setup.classify_goal_dynamics(nodes)
            self.inference_setup.assign_probative_value(nodes)
            failure_points = self.inference_setup.identify_failure_points(graph, text)
            
            # Step 7: DNP Standards Validation (if available)
            if self.dnp_validator:
                self.logger.info("Validando cumplimiento de estándares DNP...")
                self._validate_dnp_compliance(nodes, graph, policy_code)
            
            # Step 8: Generate reports
            self.logger.info("Generando reportes y visualizaciones...")
            self.reporting_engine.generate_causal_diagram(graph, policy_code)
            self.reporting_engine.generate_accountability_matrix(graph, policy_code)
            self.reporting_engine.generate_confidence_report(
                nodes, graph, self.causal_extractor.causal_chains,
                audit_results, self.financial_auditor, sequence_warnings, policy_code
            )
            self.reporting_engine.generate_causal_model_json(graph, nodes, policy_code)
            
            # Step 8: Generate Bayesian inference reports
            self.logger.info("Generando reportes de inferencia Bayesiana...")
            self._generate_bayesian_reports(
                inferred_mechanisms, counterfactual_audit, policy_code
            )
            
            # Step 9: Self-reflective learning from audit results (frontier paradigm)
            if self.config.validated_config and self.config.validated_config.self_reflection.enable_prior_learning:
                self.logger.info("Actualizando priors con retroalimentación del análisis...")
                feedback_data = self._extract_feedback_from_audit(
                    inferred_mechanisms, counterfactual_audit, audit_results
                )
                self.config.update_priors_from_feedback(feedback_data)
            
            self.logger.info(f"✅ Procesamiento completado exitosamente para {policy_code}")
            return True
            
        except CDAFException as e:
            # Structured error handling with custom exceptions
            self.logger.error(f"Error CDAF: {e.message}")
            self.logger.error(f"Detalles: {json.dumps(e.to_dict(), indent=2)}")
            if not e.recoverable:
                raise
            return False
        except Exception as e:
            # Wrap unexpected errors in CDAFProcessingError
            raise CDAFProcessingError(
                "Error crítico en el procesamiento",
                details={'error': str(e), 'type': type(e).__name__},
                stage="document_processing",
                recoverable=False
            ) from e
    
    def _extract_feedback_from_audit(self, inferred_mechanisms: Dict[str, Dict[str, Any]],
                                     counterfactual_audit: Dict[str, Any],
                                     audit_results: Dict[str, AuditResult]) -> Dict[str, Any]:
        """
        Extract feedback data from audit results for self-reflective prior updating
        
        This implements the frontier paradigm of learning from audit results
        to improve future inference accuracy.
        
        HARMONIC FRONT 4 ENHANCEMENT:
        - Reduces mechanism_type_priors for mechanisms with implementation_failure flags
        - Tracks necessity/sufficiency test failures
        - Penalizes "miracle" mechanisms that fail counterfactual tests
        """
        feedback = {}
        
        # Extract mechanism type frequencies from successful inferences
        mechanism_frequencies = defaultdict(float)
        failure_frequencies = defaultdict(float)  # NEW: Track failures
        total_mechanisms = 0
        total_failures = 0
        
        # Get causal implications from audit
        causal_implications = counterfactual_audit.get('causal_implications', {})
        
        for node_id, mechanism in inferred_mechanisms.items():
            mechanism_type_dist = mechanism.get('mechanism_type', {})
            # Weight by confidence (coherence score)
            confidence = mechanism.get('coherence_score', 0.5)
            
            # Check for implementation_failure flags in audit results
            node_implications = causal_implications.get(node_id, {})
            causal_effects = node_implications.get('causal_effects', {})
            has_implementation_failure = 'implementation_failure' in causal_effects
            
            # Check necessity/sufficiency test results
            necessity_test = mechanism.get('necessity_test', {})
            sufficiency_test = mechanism.get('sufficiency_test', {})
            failed_necessity = not necessity_test.get('is_necessary', True)
            failed_sufficiency = not sufficiency_test.get('is_sufficient', True)
            
            # If mechanism failed tests or has implementation_failure flag
            if has_implementation_failure or failed_necessity or failed_sufficiency:
                total_failures += 1
                # Track which mechanism types are associated with failures
                for mech_type, prob in mechanism_type_dist.items():
                    failure_frequencies[mech_type] += prob * confidence
            else:
                # Only count successes for positive reinforcement
                for mech_type, prob in mechanism_type_dist.items():
                    mechanism_frequencies[mech_type] += prob * confidence
                    total_mechanisms += confidence
        
        # Normalize frequencies
        if total_mechanisms > 0:
            mechanism_frequencies = {
                k: v / total_mechanisms 
                for k, v in mechanism_frequencies.items()
            }
            feedback['mechanism_frequencies'] = dict(mechanism_frequencies)
        
        # NEW: Calculate penalty factors for failed mechanism types
        if total_failures > 0:
            failure_frequencies = {
                k: v / total_failures
                for k, v in failure_frequencies.items()
            }
            feedback['failure_frequencies'] = dict(failure_frequencies)
            
            # Calculate penalty: reduce priors for frequently failing types
            penalty_factors = {}
            for mech_type, failure_freq in failure_frequencies.items():
                # Higher failure frequency = stronger penalty (0.7 to 0.95 reduction)
                penalty_factors[mech_type] = 0.95 - (failure_freq * 0.25)
            feedback['penalty_factors'] = penalty_factors
        
        # Add audit quality metrics for future reference
        feedback['audit_quality'] = {
            'total_nodes_audited': len(audit_results),
            'passed_count': sum(1 for r in audit_results.values() if r['passed']),
            'success_rate': sum(1 for r in audit_results.values() if r['passed']) / max(len(audit_results), 1),
            'failure_count': total_failures,  # NEW
            'failure_rate': total_failures / max(len(inferred_mechanisms), 1)  # NEW
        }
        
        # Track necessity/sufficiency failures for iterative validation loop
        necessity_failures = sum(1 for m in inferred_mechanisms.values() 
                                if not m.get('necessity_test', {}).get('is_necessary', True))
        sufficiency_failures = sum(1 for m in inferred_mechanisms.values()
                                  if not m.get('sufficiency_test', {}).get('is_sufficient', True))
        
        feedback['test_failures'] = {
            'necessity_failures': necessity_failures,
            'sufficiency_failures': sufficiency_failures
        }
        
        return feedback
    
    def _validate_dnp_compliance(self, nodes: Dict[str, MetaNode], 
                                graph: nx.DiGraph, policy_code: str) -> None:
        """
        Validate DNP compliance for all nodes/projects
        Generates DNP compliance report
        """
        if not self.dnp_validator:
            return
        
        # Build project list from nodes
        proyectos = []
        for node_id, node in nodes.items():
            # Extract sector from responsible entity or type
            sector = "general"
            if node.responsible_entity:
                entity_lower = node.responsible_entity.lower()
                if "educaci" in entity_lower or "edu" in entity_lower:
                    sector = "educacion"
                elif "salud" in entity_lower:
                    sector = "salud"
                elif "agua" in entity_lower or "acueducto" in entity_lower:
                    sector = "agua_potable_saneamiento"
                elif ("via" in entity_lower or "vial" in entity_lower or "transporte" in entity_lower or "infraestructura" in entity_lower):
                    sector = "vias_transporte"
                elif "agr" in entity_lower or "rural" in entity_lower:
                    sector = "desarrollo_agropecuario"
            
            # Infer indicators from node type
            indicadores = []
            if node.type == "producto":
                # Map to MGA product indicators based on sector
                if sector == "educacion":
                    indicadores = ["EDU-020", "EDU-021"]
                elif sector == "salud":
                    indicadores = ["SAL-020", "SAL-021"]
                elif sector == "agua_potable_saneamiento":
                    indicadores = ["APS-020", "APS-021"]
            elif node.type == "resultado":
                # Map to MGA result indicators
                if sector == "educacion":
                    indicadores = ["EDU-001", "EDU-002"]
                elif sector == "salud":
                    indicadores = ["SAL-001", "SAL-002"]
                elif sector == "agua_potable_saneamiento":
                    indicadores = ["APS-001", "APS-002"]
            
            proyectos.append({
                "nombre": node_id,
                "sector": sector,
                "descripcion": node.text[:200] if node.text else "",
                "indicadores": indicadores,
                "presupuesto": node.financial_allocation or 0.0,
                "es_rural": "rural" in node.text.lower() if node.text else False,
                "poblacion_victimas": "v ctima" in node.text.lower() if node.text else False
            })
        
        # Validate each project
        dnp_results = []
        for proyecto in proyectos:
            resultado = self.dnp_validator.validar_proyecto_integral(
                sector=proyecto["sector"],
                descripcion=proyecto["descripcion"],
                indicadores_propuestos=proyecto["indicadores"],
                presupuesto=proyecto["presupuesto"],
                es_rural=proyecto["es_rural"],
                poblacion_victimas=proyecto["poblacion_victimas"]
            )
            dnp_results.append({
                "proyecto": proyecto["nombre"],
                "resultado": resultado
            })
        
        # Generate DNP compliance report
        self._generate_dnp_report(dnp_results, policy_code)
    
    def _generate_dnp_report(self, dnp_results: List[Dict], policy_code: str) -> None:
        """Generate comprehensive DNP compliance report"""
        report_path = self.output_dir / f"{policy_code}_dnp_compliance_report.txt"
        
        total_proyectos = len(dnp_results)
        if total_proyectos == 0:
            return
        
        # Calculate aggregate statistics
        proyectos_excelente = sum(1 for r in dnp_results 
                                 if r["resultado"].nivel_cumplimiento.value == "excelente")
        proyectos_bueno = sum(1 for r in dnp_results 
                             if r["resultado"].nivel_cumplimiento.value == "bueno")
        proyectos_aceptable = sum(1 for r in dnp_results 
                                 if r["resultado"].nivel_cumplimiento.value == "aceptable")
        proyectos_insuficiente = sum(1 for r in dnp_results 
                                    if r["resultado"].nivel_cumplimiento.value == "insuficiente")
        
        score_promedio = sum(r["resultado"].score_total for r in dnp_results) / total_proyectos
        
        # Build report
        lines = []
        lines.append("=" * 100)
        lines.append("REPORTE DE CUMPLIMIENTO DE ESTÁNDARES DNP")
        lines.append(f"Código de Política: {policy_code}")
        lines.append("=" * 100)
        lines.append("")
        
        lines.append("RESUMEN EJECUTIVO")
        lines.append("-" * 100)
        lines.append(f"Total de Proyectos/Metas Analizados: {total_proyectos}")
        lines.append(f"Score Promedio de Cumplimiento: {score_promedio:.1f}/100")
        lines.append("")
        lines.append("Distribución por Nivel de Cumplimiento:")
        lines.append(f"  • Excelente (>90%):      {proyectos_excelente:3d} ({proyectos_excelente/total_proyectos*100:5.1f}%)")
        lines.append(f"  • Bueno (75-90%):        {proyectos_bueno:3d} ({proyectos_bueno/total_proyectos*100:5.1f}%)")
        lines.append(f"  • Aceptable (60-75%):    {proyectos_aceptable:3d} ({proyectos_aceptable/total_proyectos*100:5.1f}%)")
        lines.append(f"  • Insuficiente (<60%):   {proyectos_insuficiente:3d} ({proyectos_insuficiente/total_proyectos*100:5.1f}%)")
        lines.append("")
        
        # Detailed validation per project
        lines.append("VALIDACIÓN DETALLADA POR PROYECTO/META")
        lines.append("=" * 100)
        
        for i, result_data in enumerate(dnp_results, 1):
            proyecto = result_data["proyecto"]
            resultado = result_data["resultado"]
            
            lines.append("")
            lines.append(f"{i}. {proyecto}")
            lines.append("-" * 100)
            lines.append(f"   Score: {resultado.score_total:.1f}/100 | Nivel: {resultado.nivel_cumplimiento.value.upper()}")
            
            # Competencies
            comp_status = "✓" if resultado.cumple_competencias else "✗"
            lines.append(f"   Competencias Municipales: {comp_status}")
            if resultado.competencias_validadas:
                lines.append(f"     - Aplicables: {', '.join(resultado.competencias_validadas[:3])}")
            
            # MGA Indicators
            mga_status = "✓" if resultado.cumple_mga else "✗"
            lines.append(f"   Indicadores MGA: {mga_status}")
            if resultado.indicadores_mga_usados:
                lines.append(f"     - Usados: {', '.join(resultado.indicadores_mga_usados)}")
            if resultado.indicadores_mga_faltantes:
                lines.append(f"     - Recomendados: {', '.join(resultado.indicadores_mga_faltantes)}")
            
            # PDET (if applicable)
            if resultado.es_municipio_pdet:
                pdet_status = "✓" if resultado.cumple_pdet else "✗"
                lines.append(f"   Lineamientos PDET: {pdet_status}")
                if resultado.lineamientos_pdet_cumplidos:
                    lines.append(f"     - Cumplidos: {len(resultado.lineamientos_pdet_cumplidos)}")
            
            # Critical alerts
            if resultado.alertas_criticas:
                lines.append(f"   ⚠ ALERTAS CRÍTICAS:")
                for alerta in resultado.alertas_criticas:
                    lines.append(f"     - {alerta}")
            
            # Recommendations
            if resultado.recomendaciones:
                lines.append(f"   📋 RECOMENDACIONES:")
                for rec in resultado.recomendaciones[:3]:  # Top 3
                    lines.append(f"     - {rec}")
        
        lines.append("")
        lines.append("=" * 100)
        lines.append("NORMATIVA DE REFERENCIA")
        lines.append("-" * 100)
        lines.append("• Competencias Municipales: Ley 136/1994, Ley 715/2001, Ley 1551/2012")
        lines.append("• Indicadores MGA: DNP - Metodología General Ajustada")
        lines.append("• PDET: Decreto 893/2017, Acuerdo Final de Paz")
        lines.append("=" * 100)
        
        # Write report
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(lines))
            self.logger.info(f"Reporte de cumplimiento DNP guardado en: {report_path}")
        except Exception as e:
            self.logger.error(f"Error guardando reporte DNP: {e}")


def main() -> int:
    """CLI entry point"""
    parser = argparse.ArgumentParser(
        description="CDAF v2.0 - Framework de Deconstrucción y Auditoría Causal",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplo de uso:
  python cdaf_framework.py documento.pdf --output-dir resultados/ --policy-code P1

Configuración:
  El framework busca config.yaml en el directorio actual.
  Use --config-file para especificar una ruta alternativa.
        """
    )
    
    parser.add_argument(
        "pdf_path",
        type=Path,
        help="Ruta al archivo PDF del Plan de Desarrollo Territorial"
    )
    
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("resultados_analisis"),
        help="Directorio de salida para los artefactos (default: resultados_analisis/)"
    )
    
    parser.add_argument(
        "--policy-code",
        type=str,
        required=True,
        help="Código de política para nombrar los artefactos (ej: P1, PDT_2024)"
    )
    
    parser.add_argument(
        "--config-file",
        type=Path,
        default=Path("config.yaml"),
        help="Ruta al archivo de configuración YAML (default: config.yaml)"
    )
    
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="Nivel de logging (default: INFO)"
    )
    
    parser.add_argument(
        "--pdet",
        action="store_true",
        help="Indica si el municipio es PDET (activa validación especial)"
    )
    
    args = parser.parse_args()
    
    # Validate inputs
    if not args.pdf_path.exists():
        print(f"ERROR: Archivo PDF no encontrado: {args.pdf_path}")
        return 1
    
    # Initialize framework
    try:
        framework = CDAFFramework(args.config_file, args.output_dir, args.log_level)
        
        # Configure PDET if specified
        if args.pdet and framework.dnp_validator:
            framework.dnp_validator.es_municipio_pdet = True
            framework.logger.info("Modo PDET activado - Validación especial habilitada")
    except Exception as e:
        print(f"ERROR: No se pudo inicializar el framework: {e}")
        return 1
    
    # Process document
    success = framework.process_document(args.pdf_path, args.policy_code)
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())
