]633;E;echo "=== ESTRUCTURA COMPLETA ===";866eb5b8-f560-40f8-8cd7-111ae6fa05b7]633;C=== ESTRUCTURA COMPLETA ===
total 1616
drwxrwxrwx+ 5 codespace root        4096 Oct 14 15:47 .
drwxr-xrwx+ 5 codespace root        4096 Oct 14 12:55 ..
drwxrwxrwx+ 8 codespace root        4096 Oct 14 15:39 .git
-rw-rw-rw-  1 codespace root         436 Oct 14 12:55 .gitignore
drwxrwxrwx+ 2 codespace root        4096 Oct 14 12:55 .idea
-rw-rw-rw-  1 codespace root        2134 Oct 14 12:55 AGENTS.md
-rw-rw-rw-  1 codespace root        7689 Oct 14 12:55 AUDIT_REPORT.md
-rw-rw-rw-  1 codespace root        9854 Oct 14 12:55 BAYESIAN_INFERENCE_IMPLEMENTATION.md
-rw-rw-rw-  1 codespace root        8963 Oct 14 12:55 BAYESIAN_QUICK_REFERENCE.md
-rw-rw-rw-  1 codespace root       12019 Oct 14 12:55 CANONICAL_NOTATION_DOCS.md
-rw-rw-rw-  1 codespace root        7106 Oct 14 12:55 CANONICAL_NOTATION_IMPLEMENTATION_SUMMARY.md
-rw-rw-rw-  1 codespace root        5813 Oct 14 12:55 CANONICAL_NOTATION_QUICK_REF.md
-rw-rw-rw-  1 codespace root       14755 Oct 14 12:55 CATEGORY_2_BEFORE_AFTER.md
-rw-rw-rw-  1 codespace root       11907 Oct 14 12:55 CATEGORY_2_IMPLEMENTATION.md
-rw-rw-rw-  1 codespace root        8668 Oct 14 12:55 CATEGORY_2_QUICK_REFERENCE.md
-rw-rw-rw-  1 codespace root        5323 Oct 14 12:55 COMPILATION_VALIDATION.md
-rw-rw-rw-  1 codespace root        8831 Oct 14 12:55 DATA_INTEGRITY_AND_RESOURCE_MANAGEMENT.md
-rw-rw-rw-  1 codespace root       11829 Oct 14 12:55 DEREK_ENHANCEMENTS.md
-rw-rw-rw-  1 codespace root        9490 Oct 14 12:55 DNP_INTEGRATION_DOCS.md
-rw-rw-rw-  1 codespace codespace    486 Oct 14 15:42 Dockerfile
-rw-rw-rw-  1 codespace codespace    116 Oct 14 15:47 FULL_PROJECT_INFO.txt
-rw-rw-rw-  1 codespace root        5314 Oct 14 12:55 GUIA_RAPIDA_DNP.md
-rw-rw-rw-  1 codespace root        5604 Oct 14 12:55 IMPLEMENTATION_CLEAN_SCRIPTS.md
-rw-rw-rw-  1 codespace root       10471 Oct 14 12:55 IMPLEMENTATION_SUMMARY.txt
-rw-rw-rw-  1 codespace root        7799 Oct 14 12:55 IMPLEMENTATION_SUMMARY_OLD.md
-rw-rw-rw-  1 codespace root        9323 Oct 14 12:55 ORCHESTRATION_README.md
-rw-rw-rw-  1 codespace root        2923 Oct 14 12:55 ORCHESTRATION_SUMMARY.md
-rw-rw-rw-  1 codespace root       14735 Oct 14 12:55 ORCHESTRATOR_DOCUMENTATION.md
-rw-rw-rw-  1 codespace codespace  91631 Oct 14 15:41 PROJECT_FULL_INFO.txt
-rw-rw-rw-  1 codespace root        6471 Oct 14 12:55 QUICK_START_ENHANCED.md
-rw-rw-rw-  1 codespace root        2992 Oct 14 12:55 QUICK_START_VALIDATION.md
-rw-rw-rw-  1 codespace root        7767 Oct 14 12:55 README.md
-rw-rw-rw-  1 codespace root        8975 Oct 14 12:55 RESUMEN_EJECUTIVO_DNP.md
-rw-rw-rw-  1 codespace root       12489 Oct 14 12:55 RISK_MITIGATION_DOCS.md
drwxrwxrwx+ 2 codespace codespace   4096 Oct 14 13:37 __pycache__
-rw-rw-rw-  1 codespace codespace   1926 Oct 14 13:04 aws_deployment_info.txt
-rw-rw-rw-  1 codespace codespace    660 Oct 14 15:39 buildspec.yml
-rw-rw-rw-  1 codespace root       15675 Oct 14 12:55 canonical_integration.py
-rw-rw-rw-  1 codespace root       22727 Oct 14 12:55 canonical_notation.py
-rw-rw-rw-  1 codespace codespace   1069 Oct 14 13:06 check_dependencies.py
-rw-rw-rw-  1 codespace root       18744 Oct 14 12:55 circuit_breaker.py
-rw-rw-rw-  1 codespace root       16120 Oct 14 12:55 competencias_municipales.py
-rw-rw-rw-  1 codespace root        6311 Oct 14 12:55 config_example_enhanced.yaml
-rw-rw-rw-  1 codespace root       93032 Oct 14 12:55 contradiction_deteccion
-rw-rw-rw-  1 codespace root       10699 Oct 14 12:55 demo_bayesian_agujas.py
-rw-rw-rw-  1 codespace root        9365 Oct 14 12:55 demo_category2_improvements.py
-rw-rw-rw-  1 codespace root       11004 Oct 14 12:55 demo_orchestrator.py
-rw-rw-rw-  1 codespace root        5907 Oct 14 12:55 demo_validation_and_resources.py
-rw-rw-rw-  1 codespace root      119638 Oct 14 12:55 dereck_beach
-rw-rw-rw-  1 codespace root       18066 Oct 14 12:55 dnp_integration.py
-rw-rw-rw-  1 codespace root       13689 Oct 14 12:55 ejemplo_canonical_notation.py
-rw-rw-rw-  1 codespace root        9913 Oct 14 12:55 ejemplo_dnp_completo.py
-rw-rw-rw-  1 codespace root       51411 Oct 14 12:55 embeddings_policy
-rw-rw-rw-  1 codespace codespace    900 Oct 14 13:08 estimate_processing.py
-rw-rw-rw-  1 codespace root       11020 Oct 14 12:55 example_risk_integration.py
-rw-rw-rw-  1 codespace root       58023 Oct 14 12:55 financiero_viabilidad_tablas
-rw-rw-rw-  1 codespace codespace  21013 Oct 14 14:41 get-docker.sh
-rw-rw-rw-  1 codespace root       82325 Oct 14 12:55 guia_cuestionario
-rw-rw-rw-  1 codespace root       24350 Oct 14 12:55 initial_processor_causal_policy
-rw-rw-rw-  1 codespace root       39717 Oct 14 12:55 mga_indicadores.py
-rw-rw-rw-  1 codespace root       18428 Oct 14 12:55 module_choreographer.py
-rw-rw-rw-  1 codespace root       14995 Oct 14 12:55 module_interfaces.py
-rw-rw-rw-  1 codespace root         843 Oct 14 12:55 mypy.ini
-rw-rw-rw-  1 codespace root       32448 Oct 14 12:55 orchestrator.py
-rw-rw-rw-  1 codespace root       28010 Oct 14 12:55 pdet_lineamientos.py
-rw-rw-rw-  1 codespace root       17925 Oct 14 12:55 pipeline_dag.py
-rw-rw-rw-  1 codespace root        6776 Oct 14 12:55 pipeline_validators.py
-rwxrwxrwx  1 codespace root        5294 Oct 14 12:55 pretest_compilation.py
-rwxrwxrwx  1 codespace codespace   1567 Oct 14 15:41 project_analysis.sh
-rw-rw-rw-  1 codespace root       29635 Oct 14 12:55 question_answering_engine.py
-rw-rw-rw-  1 codespace root       39202 Oct 14 12:55 questions_config.json
-rw-rw-rw-  1 codespace root       51338 Oct 14 12:55 report_generator.py
-rw-rw-rw-  1 codespace root         595 Oct 14 12:55 requirements.txt
-rw-rw-rw-  1 codespace root       28121 Oct 14 12:55 resilience_config.py
-rw-rw-rw-  1 codespace root        5642 Oct 14 12:55 resource_management.py
-rw-rw-rw-  1 codespace root       23705 Oct 14 12:55 risk_mitigation_layer.py
-rw-rw-rw-  1 codespace root       19277 Oct 14 12:55 risk_registry.py
-rw-rw-rw-  1 codespace root         876 Oct 14 12:55 script.js
-rw-rw-rw-  1 codespace root       16743 Oct 14 12:55 smart_recommendations.py
-rw-rw-rw-  1 codespace root        1628 Oct 14 12:55 styles.css
-rwxrwxrwx  1 codespace root        4467 Oct 14 12:55 system_health_check.py
-rw-rw-rw-  1 codespace root       30835 Oct 14 12:55 teoria_cambio_validacion_monte_carlo
-rw-rw-rw-  1 codespace root       18564 Oct 14 12:55 test_canonical_notation.py
-rw-rw-rw-  1 codespace root       18886 Oct 14 12:55 test_circuit_breaker.py
-rw-rw-rw-  1 codespace root        5651 Oct 14 12:55 test_config_enhancements.py
-rw-rw-rw-  1 codespace root       11991 Oct 14 12:55 test_module_interfaces.py
-rw-rw-rw-  1 codespace root       13854 Oct 14 12:55 test_pipeline_dag.py
-rw-rw-rw-  1 codespace root        7490 Oct 14 12:55 test_pipeline_validators.py
-rw-rw-rw-  1 codespace root        5081 Oct 14 12:55 test_resource_management.py
-rw-rw-rw-  1 codespace root        7530 Oct 14 12:55 test_risk_mitigation.py
-rw-rw-rw-  1 codespace root        6780 Oct 14 12:55 test_risk_registry.py
-rwxrwxrwx  1 codespace root         635 Oct 14 12:55 validate.sh

=== TODOS LOS .PY ===
./test_pipeline_validators.py
./demo_validation_and_resources.py
./orchestrator.py
./system_health_check.py
./example_risk_integration.py
./module_choreographer.py
./pipeline_validators.py
./check_dependencies.py
./test_risk_mitigation.py
./pipeline_dag.py
./module_interfaces.py
./demo_category2_improvements.py
./demo_bayesian_agujas.py
./pdet_lineamientos.py
./report_generator.py
./ejemplo_dnp_completo.py
./demo_orchestrator.py
./test_resource_management.py
./resource_management.py
./canonical_integration.py
./pretest_compilation.py
./risk_mitigation_layer.py
./competencias_municipales.py
./risk_registry.py
./test_pipeline_dag.py
./test_canonical_notation.py
./test_circuit_breaker.py
./test_module_interfaces.py
./smart_recommendations.py
./resilience_config.py
./estimate_processing.py
./circuit_breaker.py
./ejemplo_canonical_notation.py
./mga_indicadores.py
./dnp_integration.py
./test_config_enhancements.py
./question_answering_engine.py
./canonical_notation.py
./test_risk_registry.py

=== README ===
# FARFAN-2.0
Framework Avanzado de Reconstrucci√≥n y An√°lisis de Formulaciones de Acci√≥n Nacional 2.0

## Descripci√≥n

FARFAN-2.0 es un framework de grado industrial para la deconstrucci√≥n y auditor√≠a causal de Planes de Desarrollo Territorial en Colombia, con √©nfasis en cumplimiento riguroso de est√°ndares del DNP (Departamento Nacional de Planeaci√≥n).

## Caracter√≠sticas Principales

### 1. **NUEVO: Sistema de Evaluaci√≥n de 300 Preguntas**

El orquestador implementa un sistema completo de evaluaci√≥n mediante **300 preguntas causales**:

- **30 Preguntas Base**: Organizadas en 6 dimensiones del Marco L√≥gico
  - D1: Insumos (Diagn√≥stico y L√≠neas Base)
  - D2: Actividades (Formalizadas)
  - D3: Productos (Verificables)
  - D4: Resultados (Medibles)
  - D5: Impactos (Largo Plazo)
  - D6: Causalidad (Teor√≠a de Cambio)

- **10 √Åreas de Pol√≠tica** (Dec√°logo):
  - P1: Derechos de las mujeres e igualdad de g√©nero
  - P2: Prevenci√≥n de la violencia y protecci√≥n frente al conflicto
  - P3: Ambiente sano, cambio clim√°tico, prevenci√≥n y atenci√≥n a desastres
  - P4: Derechos econ√≥micos, sociales y culturales
  - P5: Derechos de las v√≠ctimas y construcci√≥n de paz
  - P6: Derecho al buen futuro de la ni√±ez, adolescencia, juventud
  - P7: Tierras y territorios
  - P8: L√≠deres y defensores de derechos humanos
  - P9: Crisis de derechos de personas privadas de la libertad
  - P10: Migraci√≥n transfronteriza

**Cada respuesta incluye**:
- Texto de respuesta directa
- Argumento de nivel doctoral (2+ p√°rrafos)
- Nota cuantitativa (0.0-1.0)
- Evidencia del documento
- M√≥dulos que contribuyeron

**Reportes a 3 Niveles**:
1. **MICRO**: 300 respuestas individuales
2. **MESO**: 4 cl√∫steres √ó 6 dimensiones
3. **MACRO**: Alineaci√≥n global + an√°lisis retrospectivo/prospectivo

### 2. Framework CDAF (Causal Deconstruction and Audit Framework)
- Extracci√≥n autom√°tica de jerarqu√≠as causales desde PDFs
- An√°lisis de mecanismos causales (Entidad-Actividad)
- Trazabilidad financiera
- Auditor√≠a de operacionalizaci√≥n
- Generaci√≥n de diagramas causales y matrices de responsabilidad

### 3. **NUEVO: Cumplimiento Integral de Est√°ndares DNP**

#### Competencias Municipales
- **17 competencias** catalogadas seg√∫n normativa colombiana
- Validaci√≥n autom√°tica de competencias propias y concurrentes
- Base legal completa (Ley 136/1994, Ley 715/2001, Ley 1551/2012)
- 14 sectores de intervenci√≥n cubiertos

#### Indicadores MGA
- **51 indicadores** del cat√°logo oficial MGA
  - 28 indicadores de producto
  - 23 indicadores de resultado
- F√≥rmulas de c√°lculo oficiales
- Fuentes de informaci√≥n verificadas
- Alineaci√≥n con ODS (Objetivos de Desarrollo Sostenible)

#### Lineamientos PDET
- **17 lineamientos** para los 170 municipios PDET
- **8 pilares** del Acuerdo de Paz implementados
- Validaci√≥n especial de participaci√≥n comunitaria
- Requisitos de inversi√≥n rural (>60%)
- Alineaci√≥n con PATR subregionales

## Instalaci√≥n

```bash
# Clonar repositorio
git clone https://github.com/kkkkknhh/FARFAN-2.0.git
cd FARFAN-2.0

# Instalar dependencias (opcional, para framework completo)
pip install pymupdf networkx pandas spacy pyyaml fuzzywuzzy python-Levenshtein pydot

# Descargar modelo spaCy espa√±ol
python -m spacy download es_core_news_lg

# Validar que todo compile correctamente
python3 pretest_compilation.py
# O de forma r√°pida:
./validate.sh
```

## Uso R√°pido

### Sistema de Orquestaci√≥n Completo (NUEVO)

El orquestador integra **todos los m√≥dulos** para evaluar planes mediante **300 preguntas**:

```bash
# Procesar un plan de desarrollo
python orchestrator.py plan_desarrollo.pdf \
    --policy-code PDM2024-ANT-MED \
    --output-dir ./resultados \
    --pdet

# Demostraci√≥n del sistema
python demo_orchestrator.py --simple
```

**Salida generada**:
- `micro_report_{code}.json` - 300 respuestas individuales
- `meso_report_{code}.json` - 4 cl√∫steres √ó 6 dimensiones
- `macro_report_{code}.json/md` - Evaluaci√≥n global

Ver [ORCHESTRATION_README.md](ORCHESTRATION_README.md) para documentaci√≥n completa.

### Validaci√≥n DNP Standalone

```python
from dnp_integration import ValidadorDNP

validador = ValidadorDNP(es_municipio_pdet=True)

resultado = validador.validar_proyecto_integral(
    sector="educacion",
    descripcion="Construcci√≥n de 5 sedes educativas en zona rural",
    indicadores_propuestos=["EDU-020", "EDU-021", "EDU-002"],
    presupuesto=2_000_000_000,
    es_rural=True,
    poblacion_victimas=True
)

print(validador.generar_reporte_cumplimiento(resultado))
```

### Framework Completo CDAF

```bash
# Procesamiento est√°ndar
python dereck_beach documento.pdf --output-dir resultados/ --policy-code PDM2024

# Procesamiento para municipio PDET
python dereck_beach documento.pdf --output-dir resultados/ --policy-code PDM2024 --pdet
```

### Ejemplos Interactivos

```bash
# Ejecutar ejemplos completos
python ejemplo_dnp_completo.py
```

## M√≥dulos

### **NUEVO: Sistema de Orquestaci√≥n Integral**
- `orchestrator.py` - Orquestador principal con flujo can√≥nico de 9 etapas
- `question_answering_engine.py` - Motor de respuesta a 300 preguntas
- `report_generator.py` - Generador de reportes micro, meso y macro
- `module_choreographer.py` - Core√≥grafo de m√≥dulos y acumulador de respuestas

### M√≥dulos DNP (Nuevos)
- `competencias_municipales.py` - Cat√°logo de competencias municipales
- `mga_indicadores.py` - Cat√°logo de indicadores MGA
- `pdet_lineamientos.py` - Lineamientos PDET
- `dnp_integration.py` - Integraci√≥n y validaci√≥n DNP
- `canonical_notation.py` - **NUEVO:** Sistema can√≥nico de notaci√≥n (P#-D#-Q#)
- `ejemplo_dnp_completo.py` - Ejemplos de uso

### M√≥dulos Framework Principal
- `dereck_beach` - Framework CDAF principal
- `initial_processor_causal_policy` - Procesador de pol√≠ticas causales
- `teoria_cambio_validacion_monte_carlo` - Validaci√≥n de teor√≠a de cambio
- `guia_cuestionario` - Cuestionario de validaci√≥n causal

## Salidas Generadas

El framework genera autom√°ticamente:

1. **{policy_code}_causal_diagram.png** - Diagrama causal visual
2. **{policy_code}_accountability_matrix.md** - Matriz de responsabilidades
3. **{policy_code}_confidence_report.json** - Reporte de confianza
4. **{policy_code}_causal_model.json** - Modelo causal estructurado
5. **{policy_code}_dnp_compliance_report.txt** - **NUEVO:** Reporte de cumplimiento DNP

## Documentaci√≥n

- [DNP Integration Documentation](DNP_INTEGRATION_DOCS.md) - Gu√≠a completa de validaci√≥n DNP
- [Canonical Notation Documentation](CANONICAL_NOTATION_DOCS.md) - **NUEVO:** Sistema can√≥nico de notaci√≥n
- Ver ejemplos en `ejemplo_dnp_completo.py`

## Est√°ndares y Normativa

### Competencias Municipales
- Constituci√≥n Pol√≠tica de Colombia (1991)
- Ley 136 de 1994 - Organizaci√≥n Municipal
- Ley 715 de 2001 - Sistema General de Participaciones
- Ley 1551 de 2012 - Modernizaci√≥n Municipal

### Indicadores MGA
- DNP - Metodolog√≠a General Ajustada (MGA)
- Sistema de Seguimiento a Proyectos de Inversi√≥n (SPI)

### PDET
- Decreto 893 de 2017 - Creaci√≥n de PDET
- Acuerdo Final para la Terminaci√≥n del Conflicto (2016)
- Agencia de Renovaci√≥n del Territorio (ART)

## Niveles de Cumplimiento DNP

- **EXCELENTE**: >90% - Cumplimiento sobresaliente
- **BUENO**: 75-90% - Cumplimiento adecuado
- **ACEPTABLE**: 60-75% - Cumplimiento m√≠nimo
- **INSUFICIENTE**: <60% - Requiere mejoras

## Contribuciones

Este proyecto implementa est√°ndares oficiales del DNP y el Acuerdo de Paz de Colombia. Las contribuciones deben mantener estricta adherencia a la normativa colombiana vigente.

## Licencia

Ver archivo LICENSE

## Contacto

Para soporte sobre est√°ndares DNP:
- DNP: https://www.dnp.gov.co
- ART: https://www.renovacionterritorio.gov.co


=== REQUIREMENTS ===
# FARFAN 2.0 - Requirements
# Colombian Territorial Development Plans Analysis Framework
# Python 3.10+ required

# Core Scientific Computing
numpy==1.24.3
scipy==1.11.4

# Deep Learning & Transformers
torch==2.1.2
transformers==4.36.2

# NLP - spaCy
spacy==3.7.2

# Data Processing
pandas==2.1.4

# Graph Analysis
networkx==3.2.1

# PDF Processing
PyMuPDF==1.23.8

# YAML Configuration
PyYAML==6.0.1

# Fuzzy String Matching
fuzzywuzzy==0.18.0
python-Levenshtein==0.25.0

# Graph Visualization
pydot==2.0.0

# Data Validation
pydantic==2.5.3

# Additional Dependencies
typing-extensions==4.9.0

=== ORCHESTRATOR.PY COMPLETO ===
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FARFAN 2.0 - Orchestrator Principal
Flujo Can√≥nico, Determinista e Inmutable para Evaluaci√≥n de Planes de Desarrollo

Este orquestador integra TODOS los m√≥dulos del framework en un flujo coherente
que eval√∫a 300 preguntas (30 preguntas √ó 10 √°reas de pol√≠tica) con:
- Nivel Micro: Respuesta individual por pregunta (300 respuestas)
- Nivel Meso: Agrupaci√≥n en 4 cl√∫steres √ó 6 dimensiones
- Nivel Macro: Evaluaci√≥n global de alineaci√≥n con el dec√°logo

Principios:
- Determinista: Siempre produce el mismo resultado para el mismo input
- Inmutable: No modifica datos originales, solo genera nuevas estructuras
- Can√≥nico: Orden de ejecuci√≥n fijo y documentado
- Exhaustivo: Usa TODAS las funciones y clases de cada m√≥dulo
"""

import logging
import json
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Any
from enum import Enum

# Import validation and resource management
from pipeline_validators import (
    DocumentProcessingData,
    SemanticAnalysisData,
    CausalExtractionData,
    MechanismInferenceData,
    FinancialAuditData,
    DNPValidationData,
    QuestionAnsweringData,
    ReportGenerationData,
    validate_stage_transition
)
from resource_management import (
    managed_stage_execution,
    MemoryMonitor
)

# Import module wiring components
from module_interfaces import DependencyInjectionContainer, CDAFAdapter
from module_choreographer import ModuleChoreographer
from pipeline_dag import create_default_pipeline

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("orchestrator")


class PipelineStage(Enum):
    """Etapas del pipeline en orden can√≥nico"""
    LOAD_DOCUMENT = 1
    EXTRACT_TEXT_TABLES = 2
    SEMANTIC_ANALYSIS = 3
    CAUSAL_EXTRACTION = 4
    MECHANISM_INFERENCE = 5
    FINANCIAL_AUDIT = 6
    DNP_VALIDATION = 7
    QUESTION_ANSWERING = 8
    REPORT_GENERATION = 9


@dataclass
class PipelineContext:
    """Contexto compartido entre etapas del pipeline"""
    # Input
    pdf_path: Path
    policy_code: str
    output_dir: Path
    
    # Stage 1-2: Document processing
    raw_text: str = ""
    sections: Dict[str, str] = field(default_factory=dict)
    tables: List[Any] = field(default_factory=list)
    
    # Stage 3: Semantic analysis
    semantic_chunks: List[Dict] = field(default_factory=list)
    dimension_scores: Dict[str, float] = field(default_factory=dict)
    
    # Stage 4: Causal extraction
    causal_graph: Any = None
    nodes: Dict[str, Any] = field(default_factory=dict)
    causal_chains: List[Dict] = field(default_factory=list)
    
    # Stage 5: Mechanism inference
    mechanism_parts: List[Dict] = field(default_factory=list)
    bayesian_inferences: Dict[str, Any] = field(default_factory=dict)
    
    # Stage 6: Financial audit
    financial_allocations: Dict[str, float] = field(default_factory=dict)
    budget_traceability: Dict[str, Any] = field(default_factory=dict)
    
    # Stage 7: DNP validation
    dnp_validation_results: List[Dict] = field(default_factory=list)
    compliance_score: float = 0.0
    
    # Stage 8: Question answering
    question_responses: Dict[str, Any] = field(default_factory=dict)
    
    # Stage 9: Reports
    micro_report: Dict = field(default_factory=dict)
    meso_report: Dict = field(default_factory=dict)
    macro_report: Dict = field(default_factory=dict)


class FARFANOrchestrator:
    """
    Orquestador principal que ejecuta el flujo completo de an√°lisis
    
    Este orquestador garantiza que:
    1. Todas las clases y funciones de cada m√≥dulo son utilizadas
    2. El flujo es determinista (mismo input ‚Üí mismo output)
    3. Los datos se transfieren de manera clara entre etapas
    4. Se genera un reporte completo a tres niveles
    """
    
    def __init__(self, output_dir: Path, log_level: str = "INFO", 
                 use_choreographer: bool = True, use_dag: bool = False):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Set log level
        logging.getLogger().setLevel(getattr(logging, log_level.upper()))
        
        # Initialize Dependency Injection Container
        self.di_container = DependencyInjectionContainer()
        
        # Initialize Module Choreographer
        self.use_choreographer = use_choreographer
        if self.use_choreographer:
            self.choreographer = ModuleChoreographer()
            logger.info("‚úì ModuleChoreographer enabled")
        else:
            self.choreographer = None
            logger.info("ModuleChoreographer disabled")
        
        # Initialize DAG-based pipeline (optional, experimental)
        self.use_dag = use_dag
        if self.use_dag:
            self.pipeline_dag = create_default_pipeline()
            logger.info("‚úì DAG-based pipeline enabled")
        else:
            self.pipeline_dag = None
        
        # Initialize all modules
        self._init_modules()
        
        # Register modules with choreographer
        if self.choreographer:
            self._register_modules_with_choreographer()
        
        logger.info("FARFANOrchestrator inicializado")
    
    def _init_modules(self):
        """Inicializa todos los m√≥dulos del framework"""
        logger.info("Inicializando m√≥dulos del framework...")
        
        # Module 1: dereck_beach (CDAF Framework)
        try:
            from dereck_beach import CDAFFramework
            from pathlib import Path
            import tempfile
            
            # Create temporary config for CDAF
            config_content = """
            nlp_model: es_core_news_lg
            confidence_thresholds:
              causal_link: 0.7
              entity_activity: 0.6
            """
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                f.write(config_content)
                temp_config = Path(f.name)
            
            self.cdaf = CDAFFramework(
                config_path=temp_config,
                output_dir=self.output_dir,
                log_level="INFO"
            )
            
            # Register CDAF components via adapter and DI container
            adapter = CDAFAdapter(self.cdaf)
            self.di_container.register('pdf_processor', adapter.get_pdf_processor())
            self.di_container.register('causal_extractor', adapter.get_causal_extractor())
            self.di_container.register('mechanism_extractor', adapter.get_mechanism_extractor())
            self.di_container.register('financial_auditor', adapter.get_financial_auditor())
            
            logger.info("‚úì CDAF Framework cargado")
        except Exception as e:
            logger.error(f"Error cargando CDAF Framework: {e}")
            self.cdaf = None
        
        # Module 2: DNP Integration
        try:
            from dnp_integration import ValidadorDNP
            self.dnp_validator = ValidadorDNP(es_municipio_pdet=False)
            self.di_container.register('dnp_validator', self.dnp_validator)
            logger.info("‚úì Validador DNP cargado")
        except Exception as e:
            logger.error(f"Error cargando Validador DNP: {e}")
            self.dnp_validator = None
        
        # Module 3: Semantic Policy Processor
        try:
            from initial_processor_causal_policy import PolicyDocumentAnalyzer
            # PolicyDocumentAnalyzer will be initialized when needed (lazy loading)
            self.policy_analyzer_class = PolicyDocumentAnalyzer
            logger.info("‚úì Policy Analyzer disponible")
        except Exception as e:
            logger.error(f"Error cargando Policy Analyzer: {e}")
            self.policy_analyzer_class = None
        
        # Module 4: Competencias Municipales
        try:
            from competencias_municipales import CatalogoCompetenciasMunicipales
            self.competencias = CatalogoCompetenciasMunicipales()
            self.di_container.register('competencias', self.competencias)
            logger.info("‚úì Cat√°logo de Competencias cargado")
        except Exception as e:
            logger.error(f"Error cargando Competencias: {e}")
            self.competencias = None
        
        # Module 5: MGA Indicators
        try:
            from mga_indicadores import CatalogoIndicadoresMGA
            self.mga_catalog = CatalogoIndicadoresMGA()
            self.di_container.register('mga_catalog', self.mga_catalog)
            logger.info("‚úì Cat√°logo MGA cargado")
        except Exception as e:
            logger.error(f"Error cargando MGA: {e}")
            self.mga_catalog = None
        
        # Module 6: PDET Lineamientos
        try:
            from pdet_lineamientos import LineamientosPDET
            self.pdet_lineamientos = LineamientosPDET()
            self.di_container.register('pdet_lineamientos', self.pdet_lineamientos)
            logger.info("‚úì Lineamientos PDET cargados")
        except Exception as e:
            logger.error(f"Error cargando PDET: {e}")
            self.pdet_lineamientos = None
        
        # Module 7: Question Answering Engine
        try:
            from question_answering_engine import QuestionAnsweringEngine
            self.qa_engine = QuestionAnsweringEngine(
                cdaf=self.cdaf,
                dnp_validator=self.dnp_validator,
                competencias=self.competencias,
                mga_catalog=self.mga_catalog,
                pdet_lineamientos=self.pdet_lineamientos
            )
            self.di_container.register('qa_engine', self.qa_engine)
            logger.info("‚úì Question Answering Engine cargado")
        except Exception as e:
            logger.error(f"Error cargando Question Answering Engine: {e}")
            self.qa_engine = None
        
        # Module 8: Report Generator
        try:
            from report_generator import ReportGenerator
            self.report_generator = ReportGenerator(output_dir=self.output_dir)
            self.di_container.register('report_generator', self.report_generator)
            logger.info("‚úì Report Generator cargado")
        except Exception as e:
            logger.error(f"Error cargando Report Generator: {e}")
            self.report_generator = None
        
        # Register choreographer in DI container
        if self.choreographer:
            self.di_container.register('choreographer', self.choreographer)
    
    def _register_modules_with_choreographer(self):
        """Register all modules with the choreographer for tracking"""
        if not self.choreographer:
            return
        
        logger.info("Registrando m√≥dulos con ModuleChoreographer...")
        
        if self.cdaf:
            self.choreographer.register_module('dereck_beach', self.cdaf)
            self.choreographer.register_module('pdf_processor', self.cdaf.pdf_processor)
            self.choreographer.register_module('causal_extractor', self.cdaf.causal_extractor)
            self.choreographer.register_module('mechanism_extractor', self.cdaf.mechanism_extractor)
            self.choreographer.register_module('financial_auditor', self.cdaf.financial_auditor)
        
        if self.dnp_validator:
            self.choreographer.register_module('dnp_validator', self.dnp_validator)
        
        if self.competencias:
            self.choreographer.register_module('competencias', self.competencias)
        
        if self.mga_catalog:
            self.choreographer.register_module('mga_catalog', self.mga_catalog)
        
        if self.pdet_lineamientos:
            self.choreographer.register_module('pdet_lineamientos', self.pdet_lineamientos)
        
        if self.qa_engine:
            self.choreographer.register_module('qa_engine', self.qa_engine)
        
        if self.report_generator:
            self.choreographer.register_module('report_generator', self.report_generator)
        
        logger.info(f"‚úì {len(self.choreographer.module_registry)} m√≥dulos registrados")
    
    def process_plan(self, pdf_path: Path, policy_code: str, 
                     es_municipio_pdet: bool = False) -> PipelineContext:
        """
        Procesa un Plan de Desarrollo completo siguiendo el flujo can√≥nico
        
        Input Contract:
            - pdf_path: Path to PDF file (must exist and be readable)
            - policy_code: Policy identifier string (e.g., "PDM2024-ANT-MED")
            - es_municipio_pdet: Boolean flag for PDET municipality status
        
        Output Contract:
            - PipelineContext with all processing results
            - question_responses: Dict with 300 question answers
            - micro_report, meso_report, macro_report: Generated reports
        
        Preconditions:
            - PDF file exists and is readable
            - All required modules are initialized
            - Output directory is writable
        
        Postconditions:
            - All stages execute successfully (or optional stages skipped)
            - Reports written to output directory
            - Execution trace available via choreographer
        
        Args:
            pdf_path: Ruta al PDF del plan
            policy_code: C√≥digo identificador del plan (ej: "PDM2024-ANT-MED")
            es_municipio_pdet: Si es municipio PDET (afecta validaciones DNP)
        
        Returns:
            PipelineContext con todos los resultados
        """
        logger.info(f"="*80)
        logger.info(f"Iniciando procesamiento de Plan: {policy_code}")
        logger.info(f"PDF: {pdf_path}")
        logger.info(f"="*80)
        
        # Initialize memory monitor
        memory_monitor = MemoryMonitor(log_interval_mb=100.0)
        
        # Initialize context
        ctx = PipelineContext(
            pdf_path=pdf_path,
            policy_code=policy_code,
            output_dir=self.output_dir
        )
        
        try:
            # STAGE 1-2: Document Loading and Extraction
            with managed_stage_execution("STAGE 1-2"):
                ctx = self._stage_extract_document(ctx)
                memory_monitor.check("After Stage 1-2")
            
            # STAGE 3: Semantic Analysis
            with managed_stage_execution("STAGE 3"):
                ctx = self._stage_semantic_analysis(ctx)
                memory_monitor.check("After Stage 3")
            
            # STAGE 4: Causal Extraction
            with managed_stage_execution("STAGE 4"):
                ctx = self._stage_causal_extraction(ctx)
                memory_monitor.check("After Stage 4")
            
            # STAGE 5: Mechanism Inference
            with managed_stage_execution("STAGE 5"):
                ctx = self._stage_mechanism_inference(ctx)
                memory_monitor.check("After Stage 5")
            
            # STAGE 6: Financial Audit
            with managed_stage_execution("STAGE 6"):
                ctx = self._stage_financial_audit(ctx)
                memory_monitor.check("After Stage 6")
            
            # STAGE 7: DNP Validation
            with managed_stage_execution("STAGE 7"):
                ctx = self._stage_dnp_validation(ctx, es_municipio_pdet)
                memory_monitor.check("After Stage 7")
            
            # STAGE 8: Question Answering (300 preguntas)
            with managed_stage_execution("STAGE 8"):
                ctx = self._stage_question_answering(ctx)
                memory_monitor.check("After Stage 8")
            
            # STAGE 9: Report Generation (Micro, Meso, Macro)
            with managed_stage_execution("STAGE 9"):
                ctx = self._stage_report_generation(ctx)
                memory_monitor.check("After Stage 9")
            
            # Generate final memory report
            memory_report = memory_monitor.report()
            
            logger.info(f"‚úÖ Procesamiento completado exitosamente para {policy_code}")
            
            # Generate execution artifacts if choreographer is enabled
            if self.choreographer:
                self._generate_execution_artifacts(policy_code)
            
        except Exception as e:
            logger.error(f"‚ùå Error en procesamiento: {e}", exc_info=True)
            raise
        
        return ctx
    
    def _generate_execution_artifacts(self, policy_code: str):
        """
        Generate execution trace and visualizations
        
        Args:
            policy_code: Policy identifier for file naming
        """
        if not self.choreographer:
            return
        
        logger.info("Generando artefactos de trazabilidad...")
        
        # Generate execution flow diagram (ASCII)
        flow_diagram = self.choreographer.generate_flow_diagram()
        flow_path = self.output_dir / f"execution_flow_{policy_code}.txt"
        with open(flow_path, 'w') as f:
            f.write(flow_diagram)
        logger.info(f"  ‚úì Diagrama de flujo: {flow_path}")
        
        # Generate Mermaid diagram
        mermaid_diagram = self.choreographer.generate_mermaid_diagram()
        mermaid_path = self.output_dir / f"execution_mermaid_{policy_code}.md"
        with open(mermaid_path, 'w') as f:
            f.write("# Diagrama de Ejecuci√≥n Real\n\n")
            f.write(mermaid_diagram)
        logger.info(f"  ‚úì Diagrama Mermaid: {mermaid_path}")
        
        # Export execution trace (JSON)
        trace = self.choreographer.export_execution_trace()
        trace_path = self.output_dir / f"execution_trace_{policy_code}.json"
        with open(trace_path, 'w') as f:
            json.dump(trace, f, indent=2)
        logger.info(f"  ‚úì Traza de ejecuci√≥n: {trace_path}")
        
        # Module usage report
        usage = self.choreographer.get_module_usage_report()
        usage_path = self.output_dir / f"module_usage_{policy_code}.json"
        with open(usage_path, 'w') as f:
            json.dump(usage, f, indent=2)
        logger.info(f"  ‚úì Reporte de uso de m√≥dulos: {usage_path}")
    
    def _stage_extract_document(self, ctx: PipelineContext) -> PipelineContext:
        """STAGE 1-2: Extrae texto, tablas y secciones del PDF"""
        logger.info(f"[STAGE 1-2] Extrayendo documento: {ctx.pdf_path}")
        
        if self.cdaf is None:
            logger.warning("CDAF no disponible, saltando extracci√≥n")
            return ctx
        
        # Use CDAF's PDF processor
        success = self.cdaf.pdf_processor.load_document(ctx.pdf_path)
        if not success:
            raise RuntimeError(f"No se pudo cargar el documento: {ctx.pdf_path}")
        
        ctx.raw_text = self.cdaf.pdf_processor.extract_text()
        ctx.tables = self.cdaf.pdf_processor.extract_tables()
        ctx.sections = self.cdaf.pdf_processor.extract_sections()
        
        # Validate extracted data
        stage_data = DocumentProcessingData(
            raw_text=ctx.raw_text,
            sections=ctx.sections,
            tables=ctx.tables
        )
        validate_stage_transition("1-2", stage_data)
        
        logger.info(f"  ‚úì Texto extra√≠do: {len(ctx.raw_text)} caracteres")
        logger.info(f"  ‚úì Tablas extra√≠das: {len(ctx.tables)}")
        logger.info(f"  ‚úì Secciones identificadas: {len(ctx.sections)}")
        
        return ctx
    
    def _stage_semantic_analysis(self, ctx: PipelineContext) -> PipelineContext:
        """STAGE 3: An√°lisis sem√°ntico del texto"""
        logger.info(f"[STAGE 3] An√°lisis sem√°ntico")
        
        # This stage uses initial_processor_causal_policy if available
        # For now, we'll create a simplified version
        # TODO: Integrate PolicyDocumentAnalyzer when available
        
        # Validate stage data
        stage_data = SemanticAnalysisData(
            semantic_chunks=ctx.semantic_chunks,
            dimension_scores=ctx.dimension_scores
        )
        validate_stage_transition("3", stage_data)
        
        logger.info(f"  ‚úì An√°lisis sem√°ntico completado (placeholder)")
        return ctx
    
    def _stage_causal_extraction(self, ctx: PipelineContext) -> PipelineContext:
        """
        STAGE 4: Extracci√≥n de jerarqu√≠a causal y grafos
        
        Input Contract:
            - ctx.raw_text: Non-empty text string
        
        Output Contract:
            - ctx.causal_graph: NetworkX DiGraph
            - ctx.nodes: Dict of MetaNode objects
            - ctx.causal_chains: List of causal links
        
        Preconditions:
            - raw_text must be extracted
            - CDAF framework must be available
        
        Postconditions:
            - Graph is a valid DAG
            - All nodes properly classified
        """
        logger.info(f"[STAGE 4] Extracci√≥n causal")
        
        if self.cdaf is None:
            logger.warning("CDAF no disponible, saltando extracci√≥n causal")
            return ctx
        
        # Execute through choreographer if available
        if self.choreographer:
            outputs = self.choreographer.execute_module_stage(
                stage_name="STAGE_4",
                module_name="causal_extractor",
                function_name="extract_causal_hierarchy",
                inputs={"text": ctx.raw_text}
            )
            ctx.causal_graph = outputs.get('result')
        else:
            # Direct execution
            ctx.causal_graph = self.cdaf.causal_extractor.extract_causal_hierarchy(ctx.raw_text)
        
        ctx.nodes = self.cdaf.causal_extractor.nodes
        ctx.causal_chains = self.cdaf.causal_extractor.causal_chains
        
        # CRITICAL VALIDATION: Post-Stage 4 must have nodes > 0
        stage_data = CausalExtractionData(
            causal_graph=ctx.causal_graph,
            nodes=ctx.nodes,
            causal_chains=ctx.causal_chains
        )
        validate_stage_transition("4", stage_data)
        
        # Add strategic assertion for causal graph integrity
        if ctx.causal_graph is not None:
            assert ctx.causal_graph.number_of_nodes() > 0, \
                "Causal graph must have at least one node"
        
        logger.info(f"  ‚úì Nodos extra√≠dos: {len(ctx.nodes)}")
        logger.info(f"  ‚úì Cadenas causales: {len(ctx.causal_chains)}")
        
        return ctx
    
    def _stage_mechanism_inference(self, ctx: PipelineContext) -> PipelineContext:
        """STAGE 5: Inferencia bayesiana de mecanismos"""
        logger.info(f"[STAGE 5] Inferencia de mecanismos")
        
        if self.cdaf is None:
            logger.warning("CDAF no disponible, saltando inferencia")
            return ctx
        
        # Extract Entity-Activity pairs and infer mechanisms
        for node_id, node in ctx.nodes.items():
            if node.type == 'producto':
                ea = self.cdaf.mechanism_extractor.extract_entity_activity(node.text)
                if ea:
                    ctx.mechanism_parts.append({
                        'node_id': node_id,
                        'entity': ea.entity,
                        'activity': ea.activity,
                        'confidence': ea.confidence
                    })
        
        # Validate stage data
        stage_data = MechanismInferenceData(
            mechanism_parts=ctx.mechanism_parts,
            bayesian_inferences=ctx.bayesian_inferences
        )
        validate_stage_transition("5", stage_data)
        
        logger.info(f"  ‚úì Pares Entidad-Actividad extra√≠dos: {len(ctx.mechanism_parts)}")
        
        return ctx
    
    def _stage_financial_audit(self, ctx: PipelineContext) -> PipelineContext:
        """STAGE 6: Auditor√≠a financiera y trazabilidad"""
        logger.info(f"[STAGE 6] Auditor√≠a financiera")
        
        if self.cdaf is None:
            logger.warning("CDAF no disponible, saltando auditor√≠a financiera")
            return ctx
        
        # Trace financial allocations
        unit_costs = self.cdaf.financial_auditor.trace_financial_allocation(
            ctx.tables, ctx.nodes
        )
        
        ctx.financial_allocations = self.cdaf.financial_auditor.financial_data
        ctx.budget_traceability = {
            'unit_costs': unit_costs,
            'total_budget': sum(ctx.financial_allocations.values())
        }
        
        # Validate stage data
        stage_data = FinancialAuditData(
            financial_allocations=ctx.financial_allocations,
            budget_traceability=ctx.budget_traceability
        )
        validate_stage_transition("6", stage_data)
        
        logger.info(f"  ‚úì Asignaciones financieras trazadas: {len(ctx.financial_allocations)}")
        
        return ctx
    
    def _stage_dnp_validation(self, ctx: PipelineContext, 
                              es_municipio_pdet: bool) -> PipelineContext:
        """STAGE 7: Validaci√≥n de est√°ndares DNP"""
        logger.info(f"[STAGE 7] Validaci√≥n DNP")
        
        if self.dnp_validator is None:
            logger.warning("Validador DNP no disponible")
            return ctx
        
        # Update PDET status
        self.dnp_validator.es_municipio_pdet = es_municipio_pdet
        
        # Validate each node as a project/goal
        for node_id, node in ctx.nodes.items():
            # Map node type to sector (simplified)
            sector = self._infer_sector(node.text)
            
            resultado = self.dnp_validator.validar_proyecto_integral(
                sector=sector,
                descripcion=node.text[:200] if node.text else "",
                indicadores_propuestos=[],  # TODO: extract from node
                presupuesto=node.financial_allocation or 0.0,
                es_rural=False,  # TODO: detect from context
                poblacion_victimas=False  # TODO: detect from context
            )
            
            ctx.dnp_validation_results.append({
                'node_id': node_id,
                'resultado': resultado
            })
        
        # Calculate compliance score
        if ctx.dnp_validation_results:
            try:
                ctx.compliance_score = sum(
                    r['resultado'].score_total for r in ctx.dnp_validation_results
                    if r.get('resultado') and hasattr(r['resultado'], 'score_total')
                ) / len(ctx.dnp_validation_results) if ctx.dnp_validation_results else 0
            except (AttributeError, TypeError, KeyError) as e:
                logger.warning(f"Error calculating compliance score: {e}")
                ctx.compliance_score = 0.0
        
        # CRITICAL VALIDATION: Post-Stage 7 compliance_score must be 0-100
        stage_data = DNPValidationData(
            dnp_validation_results=ctx.dnp_validation_results,
            compliance_score=ctx.compliance_score
        )
        validate_stage_transition("7", stage_data)
        
        logger.info(f"  ‚úì Validaciones DNP completadas: {len(ctx.dnp_validation_results)}")
        logger.info(f"  ‚úì Score de cumplimiento: {ctx.compliance_score:.1f}/100")
        
        return ctx
    
    def _stage_question_answering(self, ctx: PipelineContext) -> PipelineContext:
        """STAGE 8: Respuesta a las 300 preguntas"""
        logger.info(f"[STAGE 8] Respondiendo 300 preguntas")
        
        if self.qa_engine is None:
            logger.warning("Question Answering Engine no disponible")
            return ctx
        
        # Use the QuestionAnsweringEngine
        ctx.question_responses = self.qa_engine.answer_all_questions(ctx)
        
        # CRITICAL VALIDATION: Post-Stage 8 must have exactly 300 question responses
        stage_data = QuestionAnsweringData(
            question_responses=ctx.question_responses
        )
        validate_stage_transition("8", stage_data)
        
        logger.info(f"  ‚úì Preguntas respondidas: {len(ctx.question_responses)}")
        
        return ctx
    
    def _stage_report_generation(self, ctx: PipelineContext) -> PipelineContext:
        """STAGE 9: Generaci√≥n de reportes (Micro, Meso, Macro)"""
        logger.info(f"[STAGE 9] Generando reportes")
        
        if self.report_generator is None:
            logger.warning("Report Generator no disponible")
            return ctx
        
        # Generate reports at all levels
        ctx.micro_report = self.report_generator.generate_micro_report(
            ctx.question_responses, ctx.policy_code
        )
        
        ctx.meso_report = self.report_generator.generate_meso_report(
            ctx.question_responses, ctx.policy_code
        )
        
        ctx.macro_report = self.report_generator.generate_macro_report(
            ctx.question_responses, ctx.compliance_score, ctx.policy_code
        )
        
        # Validate stage data
        stage_data = ReportGenerationData(
            micro_report=ctx.micro_report,
            meso_report=ctx.meso_report,
            macro_report=ctx.macro_report
        )
        validate_stage_transition("9", stage_data)
        
        logger.info(f"  ‚úì Reporte Micro: {len(ctx.micro_report)} preguntas")
        logger.info(f"  ‚úì Reporte Meso: {len(ctx.meso_report)} cl√∫steres")
        logger.info(f"  ‚úì Reporte Macro generado")
        
        return ctx
    
    def _infer_sector(self, text: str) -> str:
        """Infiere el sector de pol√≠tica de un texto (simplificado)"""
        text_lower = text.lower()
        
        if any(word in text_lower for word in ['educaci√≥n', 'educativo', 'escolar', 'estudiante']):
            return 'educacion'
        elif any(word in text_lower for word in ['salud', 'hospital', 'm√©dico', 'enfermedad']):
            return 'salud'
        elif any(word in text_lower for word in ['agua', 'acueducto', 'saneamiento', 'alcantarillado']):
            return 'agua_potable_saneamiento'
        elif any(word in text_lower for word in ['seguridad', 'polic√≠a', 'convivencia']):
            return 'seguridad_convivencia'
        elif any(word in text_lower for word in ['vivienda', 'habitacional', 'hogar']):
            return 'vivienda'
        else:
            return 'general'


def main():
    """Entry point for command-line usage"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="FARFAN 2.0 - Orchestrator Principal para Evaluaci√≥n de Planes de Desarrollo"
    )
    parser.add_argument("pdf_path", type=Path, help="Ruta al PDF del plan de desarrollo")
    parser.add_argument("--policy-code", required=True, help="C√≥digo identificador del plan")
    parser.add_argument("--output-dir", type=Path, default="./resultados", 
                       help="Directorio de salida para reportes")
    parser.add_argument("--pdet", action="store_true", 
                       help="Indicar si es municipio PDET")
    parser.add_argument("--log-level", default="INFO", 
                       choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                       help="Nivel de logging")
    
    args = parser.parse_args()
    
    # Create orchestrator
    orchestrator = FARFANOrchestrator(
        output_dir=args.output_dir,
        log_level=args.log_level
    )
    
    # Process plan
    try:
        context = orchestrator.process_plan(
            pdf_path=args.pdf_path,
            policy_code=args.policy_code,
            es_municipio_pdet=args.pdet
        )
        
        print(f"\n{'='*80}")
        print(f"‚úÖ PROCESAMIENTO COMPLETADO")
        print(f"{'='*80}")
        print(f"C√≥digo de Pol√≠tica: {context.policy_code}")
        print(f"Preguntas respondidas: {len(context.question_responses)}")
        print(f"Score de cumplimiento DNP: {context.compliance_score:.1f}/100")
        print(f"Reportes generados en: {args.output_dir}")
        print(f"  - Micro: {args.output_dir}/micro_report_{context.policy_code}.json")
        print(f"  - Meso: {args.output_dir}/meso_report_{context.policy_code}.json")
        print(f"  - Macro: {args.output_dir}/macro_report_{context.policy_code}.md")
        print(f"{'='*80}\n")
        
        return 0
        
    except Exception as e:
        logger.error(f"Error en procesamiento: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())

=== MAIN/APP FILES ===
No main/app

=== CONFIGURACI√ìN ===
# CDAF Configuration File - Enhanced Version 2.0
# This demonstrates the new externalized configuration capabilities

# ============================================================================
# Document Processing Patterns
# ============================================================================
patterns:
  section_titles: '^(?:CAP√çTULO|ART√çCULO|PARTE)\s+[\dIVX]+'
  goal_codes: '[MP][RIP]-\d{3}'
  numeric_formats: '[\d,]+(?:\.\d+)?%?'
  table_headers: '(?:PROGRAMA|META|INDICADOR|L√çNEA BASE|VALOR ESPERADO)'
  financial_headers: '(?:PRESUPUESTO|VALOR|MONTO|INVERSI√ìN)'

# ============================================================================
# Lexicons for NLP Processing
# ============================================================================
lexicons:
  causal_logic:
    - 'gracias a'
    - 'con el fin de'
    - 'para lograr'
    - 'mediante'
    - 'a trav√©s de'
    - 'como resultado de'
    - 'debido a'
    - 'porque'
    - 'por medio de'
    - 'permitir√°'
    - 'contribuir√° a'
  
  goal_classification:
    tasa: 'decreciente'
    √≠ndice: 'constante'
    n√∫mero: 'suma'
    porcentaje: 'constante'
    cantidad: 'suma'
    cobertura: 'suma'
  
  contextual_factors:
    - 'riesgo'
    - 'amenaza'
    - 'obst√°culo'
    - 'limitaci√≥n'
    - 'restricci√≥n'
    - 'desaf√≠o'
    - 'brecha'
    - 'd√©ficit'
    - 'vulnerabilidad'
    - 'hip√≥tesis alternativa'
  
  administrative_keywords:
    - 'gesti√≥n'
    - 'administraci√≥n'
    - 'coordinaci√≥n'
    - 'regulaci√≥n'
    - 'normativa'
    - 'institucional'
    - 'gobernanza'
    - 'reglamento'
    - 'decreto'
    - 'resoluci√≥n'
    - 'acuerdo'

# ============================================================================
# Entity Name Aliases
# ============================================================================
entity_aliases:
  SEC GOB: 'Secretar√≠a de Gobierno'
  SEC PLAN: 'Secretar√≠a de Planeaci√≥n'
  SEC HAC: 'Secretar√≠a de Hacienda'
  SEC SALUD: 'Secretar√≠a de Salud'
  SEC EDU: 'Secretar√≠a de Educaci√≥n'
  SEC INFRA: 'Secretar√≠a de Infraestructura'

# ============================================================================
# Verb Sequences for Temporal Coherence
# ============================================================================
verb_sequences:
  diagnosticar: 1
  identificar: 2
  analizar: 3
  dise√±ar: 4
  planificar: 5
  implementar: 6
  ejecutar: 7
  monitorear: 8
  evaluar: 9

# ============================================================================
# Bayesian Inference Thresholds (EXTERNALIZED)
# ============================================================================
# These values were previously hardcoded in the source code.
# Now they can be tuned for different document types or domains.
bayesian_thresholds:
  # KL divergence threshold for convergence detection
  # Lower values = stricter convergence requirement
  # Range: [0.0, 1.0], Default: 0.01
  kl_divergence: 0.01
  
  # Minimum evidence count before checking convergence
  # Prevents premature convergence with sparse data
  # Range: [1, inf], Default: 2
  convergence_min_evidence: 2
  
  # Beta distribution prior parameters
  # Higher values = stronger prior belief
  # Range: [0.1, inf], Default: 2.0
  prior_alpha: 2.0
  prior_beta: 2.0
  
  # Laplace smoothing for likelihood calculations
  # Prevents zero probabilities with unseen data
  # Range: [0.0, inf], Default: 1.0
  laplace_smoothing: 1.0

# ============================================================================
# Mechanism Type Prior Probabilities (EXTERNALIZED)
# ============================================================================
# These priors represent domain knowledge about mechanism type frequencies
# in Colombian territorial development plans. They can be learned from data.
# MUST sum to 1.0
mechanism_type_priors:
  administrativo: 0.30  # Administrative mechanisms
  tecnico: 0.25         # Technical mechanisms
  financiero: 0.20      # Financial mechanisms
  politico: 0.15        # Political mechanisms
  mixto: 0.10           # Mixed mechanisms

# ============================================================================
# Performance and Optimization Settings
# ============================================================================
performance:
  # Enable vectorized numpy operations (faster but uses more memory)
  enable_vectorized_ops: true
  
  # Enable async processing for large PDFs (experimental)
  # Requires asyncio-compatible environment
  enable_async_processing: false
  
  # Maximum context length for spaCy processing
  # Shorter = faster but less context, Range: [100, inf]
  max_context_length: 1000
  
  # Cache spaCy embeddings to avoid recomputation
  # Recommended for production, uses ~200MB memory per 1000 nodes
  cache_embeddings: true

# ============================================================================
# Self-Reflective Learning Configuration (FRONTIER PARADIGM)
# ============================================================================
# This enables the system to learn from audit feedback and improve
# its Bayesian priors over time - a self-improving AI system.
self_reflection:
  # Enable learning from audit results to update priors
  enable_prior_learning: false
  
  # Weight for feedback in prior updates
  # 0.0 = ignore feedback, 1.0 = fully replace priors with feedback
  # Range: [0.0, 1.0], Recommended: 0.05-0.2 for gradual learning
  feedback_weight: 0.1
  
  # Path to save/load historical priors (null = don't save)
  # Example: './data/prior_history.json'
  prior_history_path: null
  
  # Minimum documents processed before applying learned priors
  # Prevents overfitting to small samples
  # Range: [1, inf], Recommended: 5-10
  min_documents_for_learning: 5

# ============================================================================
# Notes on Configuration
# ============================================================================
# 1. All numeric thresholds now use Pydantic validation for type safety
# 2. Invalid configurations are caught at load time, not runtime
# 3. Performance settings allow tuning speed vs. accuracy tradeoff
# 4. Self-reflection enables continuous improvement from audit feedback
# 5. This configuration is validated against CDAFConfigSchema (Pydantic)

=== S3 BUCKET CONTENTS ===
2025-10-13 19:46:44    5811166 plans/PLANES DE DESARROLLO/CACERES - PLAN DE DESARROLLO.pdf
2025-10-13 19:46:31    3517049 plans/PLANES DE DESARROLLO/CALOTO - PLAN DE DESARROLLO.pdf
2025-10-13 19:46:22    4768275 plans/PLANES DE DESARROLLO/CARTAGENA - PLAN DE DESARROLLO.pdf
2025-10-13 19:47:19    3541629 plans/PLANES DE DESARROLLO/CARTAGENA DEL CHAIRA - PLAN DE DESARROLLO.pdf
2025-10-13 19:45:58   11847782 plans/PLANES DE DESARROLLO/CHAPARRAL - PLAN DE DESARROLLO.pdf
2025-10-13 19:47:45    6999618 plans/PLANES DE DESARROLLO/CORINTO - PLAN DE DESARROLLO.pdf
2025-10-13 19:45:58    1652304 plans/PLANES DE DESARROLLO/EL BAGRE - PLAN DE DESARROLLO.pdf
2025-10-13 19:45:58   13825219 plans/PLANES DE DESARROLLO/EL CHARCO - PLAN DE DESARROLLO.pdf
2025-10-13 19:46:07   18195887 plans/PLANES DE DESARROLLO/EL TAMBO - PLAN DE DESARROLLO.pdf
2025-10-13 19:47:41    6185148 plans/PLANES DE DESARROLLO/EL TARRA - PLAN DE DESARROLLO.pdf
2025-10-13 19:49:13    3214022 plans/PLANES DE DESARROLLO/FLORENCIA - PLAN DE DESARROLLO.pdf
2025-10-13 19:49:21   11615929 plans/PLANES DE DESARROLLO/ITUANGO - PLAN DE DESARROLLO.pdf
2025-10-13 19:49:53    3350606 plans/PLANES DE DESARROLLO/LA MACARENA - PLAN DE DESARROLLO.pdf
2025-10-13 19:49:55    9259264 plans/PLANES DE DESARROLLO/LA MONTANÃÉITA - PLAN DE DESARROLLO.pdf
2025-10-13 19:50:16    8884531 plans/PLANES DE DESARROLLO/LOPEZ DE MICAY - PLAN DE DESARROLLO.pdf
2025-10-13 19:50:27    3377253 plans/PLANES DE DESARROLLO/MAGUI PAYAN - PLAN DE DESARROLLO.pdf
2025-10-13 19:51:53    6306656 plans/PLANES DE DESARROLLO/MIRANDA - PLAN DE DESARROLLO.pdf
2025-10-13 19:50:33    9727615 plans/PLANES DE DESARROLLO/MONTELIBANO - PLAN DE DESARROLLO.pdf
2025-10-13 19:52:02    8200803 plans/PLANES DE DESARROLLO/NECHI - PLAN DE DESARROLLO.pdf
2025-10-13 19:50:49    4293458 plans/PLANES DE DESARROLLO/OLAYA HERRERA - PLAN DE DESARROLLO.pdf
2025-10-13 19:51:32    3212505 plans/PLANES DE DESARROLLO/ORITO - PLAN DE DESARROLLO.pdf
2025-10-13 19:50:49    9363442 plans/PLANES DE DESARROLLO/PLANADAS - PLAN DE DESARROLLO.pdf
2025-10-13 19:51:47    4708777 plans/PLANES DE DESARROLLO/PUERTO ASIS - PLAN DE DESARROLLO.pdf
2025-10-13 19:51:19   90259651 plans/PLANES DE DESARROLLO/PUERTO CAICEDO - PLAN DE DESARROLLO.pdf
2025-10-13 19:53:35    4292131 plans/PLANES DE DESARROLLO/PUERTO GUZMAN - PLAN DE DESARROLLO.pdf
2025-10-13 19:53:55   12902577 plans/PLANES DE DESARROLLO/PUERTO LEGUIZAMON - PLAN DE DESARROLLO.pdf
2025-10-13 19:54:33   14345726 plans/PLANES DE DESARROLLO/PUERTO LIBERTADOR - PLAN DE DESARROLLO.pdf
2025-10-13 19:55:03   19413040 plans/PLANES DE DESARROLLO/PUERTO RICO - META - PLAN DE DESARROLLO.pdf
2025-10-13 19:57:16    3664181 plans/PLANES DE DESARROLLO/PUERTO RICO, CAQUETA - PROGRAMA DE GOBIERNO.pdf
2025-10-13 19:56:45    3017380 plans/PLANES DE DESARROLLO/QUIBDO - PLAN DE DESARROLLO.pdf
